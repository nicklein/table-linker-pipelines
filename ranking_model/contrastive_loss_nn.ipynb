{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5dd0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import \n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578b73b",
   "metadata": {},
   "source": [
    "I assume that the candidate generation and feature genration has already be run on the training and dev tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6bc381",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "##File paths\n",
    "\n",
    "# working_path = os.path.abspath('../../contrastive_loss_nn_data')\n",
    "# working_path = os.path.abspath('../../contrastive_loss_nn_w_embedding_scores')\n",
    "# working_path = os.path.abspath('../../contrastive_loss_nn_w_embedding_scores_8_12_21')\n",
    "# working_path = os.path.abspath('../../contrastive_loss_nn_embed_scores_and_values_8_16_21')\n",
    "# working_path = os.path.abspath('../../contrastive_loss_nn_data_25-75_centroid_folds')\n",
    "# working_path = os.path.abspath('../../contrastive_loss_nn_w_embedding_scores_9_9_21_v2')\n",
    "# working_path = os.path.abspath('../../contrastive_loss_nn_train_dev_test_25_75')\n",
    "# working_path = os.path.abspath('../../SemTabR4_2_0-25_75-v3.1')\n",
    "working_path = os.path.abspath('../../t2dv2_25_75_train_dev_test-v2')\n",
    "\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/t2dv2-train-lof-original-tfidf\n",
    "# train_path = f'{working_path}/t2dv2-train-lof-original-tfidf/'\n",
    "# train_path = f'{working_path}/t2dv2-train-lof-original-tfidf-HAS/'\n",
    "# train_path = f'{working_path}/t2dv2-train-lof-embed_scores_and_values/'\n",
    "# train_path = f'{working_path}/t2dv2-train-lof-original-tfidf_25-75'\n",
    "train_path = f'{working_path}/train'\n",
    "\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/t2dv2-dev-lof-original-tfidf\n",
    "# dev_path = f'{working_path}/t2dv2-dev-lof-original-tfidf/'\n",
    "# dev_path = f'{working_path}/t2dv2-dev-lof-original-tfidf-HAS/'\n",
    "# dev_path = f'{working_path}/t2dv2-dev-lof-embed_scores_and_values/'\n",
    "# dev_path = f'{working_path}/sample-t2dv2-train-lof-embed_scores_and_values/'\n",
    "# dev_path = f'{working_path}/t2dv2-dev-lof-original-tfidf_25-75'\n",
    "dev_path = f'{working_path}/dev'\n",
    "\n",
    "test_path = f'{working_path}/test'\n",
    "\n",
    "fold_dir_name = \"fold_1\"\n",
    "\n",
    "features_to_use = [\"String_Sim4\", \"ComplEx\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "4a74217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f\"{train_path}/{fold_dir_name}/evaluation_rows/\"\n",
    "dev_path = f\"{dev_path}/{fold_dir_name}/evaluation_rows/\"\n",
    "test_path = f\"{test_path}/{fold_dir_name}/evaluation_rows/\"\n",
    "\n",
    "# Features to use\n",
    "feature_name_to_cols = {\"String_Sim4\" : ['monge_elkan','jaro_winkler','levenshtein','retrieval_score'],\n",
    "                        \"String_Sim_w_Context6\" : ['monge_elkan','jaro_winkler','levenshtein','retrieval_score',\n",
    "                                                   'monge_elkan_aliases','singleton'],\n",
    "                        \"Standard7\" : ['monge_elkan','jaro_winkler','levenshtein','retrieval_score',\n",
    "                                       'monge_elkan_aliases','singleton','pagerank'],\n",
    "                        \"Pagerank\" : ['pagerank'],\n",
    "                        \"Profile_ComplEx\" : ['Profile-ComplEx-embedding-score'],\n",
    "                        \"Profile_TransE\" : ['Profile-TransE-embedding-score'],\n",
    "                        \"ComplEx\" : ['ComplEx-embedding-score'],\n",
    "                        \"TransE\" : ['TransE-embedding-score'],\n",
    "                        \"Profile\" : ['profile-score'],\n",
    "                        \"H\" : ['H_5x8-embedding-score'],\n",
    "                        \"A\" : ['A-embedding-score'],\n",
    "                        \"S\" : ['S-embedding-score'],\n",
    "                        \"TFIDF_Profile\" : ['TFIDF-Profile-score'],\n",
    "                        \"D_over_.2_Profile\" : ['D>.2-Profile-score']\n",
    "                       }\n",
    "\n",
    "train_features = [col for feature_name in features_to_use for col in feature_name_to_cols[feature_name]]\n",
    "features_including_eval_label = train_features if 'evaluation_label' in train_features else train_features + ['evaluation_label']\n",
    "\n",
    "# **NAME WORKING PATH DESCRIBING FEATURES USED HERE\n",
    "working_path = f\"{working_path}/{'-'.join(features_to_use)}/{fold_dir_name}\"\n",
    "\n",
    "#Note: One would need just the train_path and dev_path to run this notebook. The below files are output after \n",
    "# running the notebook.\n",
    "\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/training_data/pos_features.pkl\n",
    "pos_output = f'{working_path}/training_data/pos_features.pkl'\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/training_data/neg_features.pkl\n",
    "neg_output = f'{working_path}/training_data/neg_features.pkl'\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/training_data/normalization_factor.pkl\n",
    "min_max_scaler_path = f'{working_path}/training_data/normalization_factor.pkl'\n",
    "dev_output_predictions = f'{working_path}/dev_predictions/'\n",
    "test_output_predictions = f'{working_path}/test_predictions/'\n",
    "#GDrive Path: /table-linker-datasets/2019-iswc_challenge_data/t2dv2/canonical-with-context/contrastive_loss_nn_data/saved_models\n",
    "model_save_path = f'{working_path}/saved_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "a39ebedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {dev_output_predictions}\n",
    "!mkdir -p {test_output_predictions}\n",
    "!mkdir -p {model_save_path}\n",
    "!mkdir -p {working_path}/training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db26ca",
   "metadata": {},
   "source": [
    "old 'Specify train features'\n",
    "The features you specify must be present as columns in the files found in the above specified train / dev paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "c7417d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "# train_features = ['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "#         'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "#        'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "#        'lof-graph-embedding-score', 'lof-reciprocal-rank']\n",
    "\n",
    "# original plus eval label\n",
    "# train_features = ['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "#         'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "#        'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "#        'lof-graph-embedding-score', 'lof-reciprocal-rank',\n",
    "#                   'evaluation_label_copy']\n",
    "\n",
    "# original plus h, a, s embedding scores (lof-centroid)\n",
    "# train_features = ['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "#         'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "#        'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "#        'lof-graph-embedding-score', 'lof-reciprocal-rank',\n",
    "#             'lof-H_5x8-embedding-score', 'lof-A-embedding-score', 'lof-S-embedding-score']\n",
    "\n",
    "# original plus Profile-ComplEx embedding scores (lof-centroid)\n",
    "# train_features = ['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "#         'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "#        'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "#        'lof-graph-embedding-score', 'lof-reciprocal-rank',\n",
    "#                  'lof-Profile-ComplEx-embedding-score']\n",
    "\n",
    "# original plus Profile-ComplEx embedding scores (lof-centroid) minus other embeddings\n",
    "# train_features = ['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "#                   'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "#                   'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score', 'lof-reciprocal-rank',\n",
    "#                   'lof-Profile-ComplEx-embedding-score']\n",
    "\n",
    "# original minus other embeddings\n",
    "# train_features = ['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "#                   'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "#                   'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score', 'lof-reciprocal-rank']\n",
    "\n",
    "# original \"standard\" features\n",
    "# train_features = ['pagerank','retrieval_score','monge_elkan','jaro_winkler','levenshtein']\n",
    "\n",
    "# \"standard\" features + profile-ComplEx embedding scores\n",
    "# train_features = ['pagerank','retrieval_score','monge_elkan','jaro_winkler','levenshtein',\n",
    "#                   'Profile-ComplEx-embedding-score']\n",
    "\n",
    "# # original \"standard\" features + profile scores\n",
    "# train_features = ['pagerank','retrieval_score','monge_elkan','jaro_winkler','levenshtein',\n",
    "#                   'profile-score']\n",
    "\n",
    "# # original \"standard\" + Profile-ComplEx embedding *values*\n",
    "# train_features = ['pagerank','retrieval_score','monge_elkan','jaro_winkler','levenshtein']\n",
    "# # TODO - make this neater, no magic numbers\n",
    "# train_features.extend([f\"cand-Profile-ComplEx-emb-dim-{i}\" for i in range(100)])\n",
    "# train_features.extend([f\"centroid-Profile-ComplEx-emb-dim-{i}\" for i in range(100)])\n",
    "\n",
    "# just profile-complEx embedding scores\n",
    "# train_features = ['Profile-ComplEx-embedding-score']\n",
    "\n",
    "# train_features = ['evaluation_label_copy']\n",
    "\n",
    "# features_including_eval_label = train_features if 'evaluation_label' in train_features else train_features + ['evaluation_label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4fc093",
   "metadata": {},
   "source": [
    "### Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "ce2b2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(args):\n",
    "    datapath = args.train_path\n",
    "    eval_file_names = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(datapath):\n",
    "        for fn in filenames:\n",
    "            if \"csv\" not in fn:\n",
    "                continue\n",
    "            abs_fn = dirpath + fn\n",
    "            if not os.path.isfile(abs_fn):\n",
    "                print(abs_fn)\n",
    "            assert os.path.isfile(abs_fn)\n",
    "            if os.path.getsize(abs_fn) == 0:\n",
    "                continue\n",
    "            eval_file_names.append(abs_fn)\n",
    "    df_list = []\n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list) \n",
    "\n",
    "def compute_normalization_factor(args, all_data):\n",
    "    features = train_features\n",
    "    \n",
    "    min_max_scaler_path = args.min_max_scaler_path\n",
    "    all_data_features = all_data[features]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_data_features)\n",
    "    pickle.dump(scaler, open(min_max_scaler_path, 'wb'))\n",
    "    return scaler\n",
    "\n",
    "def generate_train_data(args):\n",
    "    scaler_path = args.min_max_scaler_path\n",
    "    scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "    final_list = []\n",
    "    \n",
    "    features = features_including_eval_label\n",
    "    normalize_features = train_features\n",
    "    \n",
    "    evaluation_label = ['evaluation_label']\n",
    "    positive_features_final = []\n",
    "    negative_features_final = []\n",
    "    for i,file in enumerate(tqdm(glob.glob(args.train_path + '/*.csv'))):\n",
    "        file_name = file.split('/')[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        print(file_name)\n",
    "        d_sample = pd.read_csv(file)\n",
    "        grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "        for cell in grouped_obj:\n",
    "            cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "            pos_features = []\n",
    "            neg_features = []\n",
    "            a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "            if a.empty:\n",
    "                continue\n",
    "                \n",
    "            # new code\n",
    "#             pos_rows = cell[1][(cell[1]['evaluation_label'].astype(int) == 1)][normalize_features].to_numpy()\n",
    "#             for i in range(len(pos_rows)):\n",
    "#                 pos_features.append(pos_rows[i])\n",
    "#             neg_rows = cell[1][(cell[1]['evaluation_label'].astype(int) == -1)][normalize_features].to_numpy()\n",
    "#             for i in range(min(50,len(neg_rows))):\n",
    "#                 neg_features.append(neg_rows[i])\n",
    "#             random.shuffle(pos_features)\n",
    "#             random.shuffle(neg_features)\n",
    "#             positive_features_final.append(pos_features)\n",
    "#             negative_features_final.append(neg_features)\n",
    "            \n",
    "#     print(len(positive_features_final), len(positive_features_final[3]))\n",
    "#     print(len(negative_features_final), len(negative_features_final[3]))\n",
    "#     pickle.dump(positive_features_final,open(args.pos_output,'wb'))\n",
    "#     pickle.dump(negative_features_final,open(args.neg_output,'wb'))    \n",
    "                \n",
    "                \n",
    "                \n",
    "            # -------------------------- OLD CODE ---------------------------\n",
    "            num_rows = 64\n",
    "            pos_row = a[features].drop('evaluation_label',axis=1)\n",
    "            negatives_filtered = cell[1][cell[1]['evaluation_label'] == -1]\n",
    "            \n",
    "            #TODO -- MAKE THIS BETTER\n",
    "            \n",
    "            negatives_filtered = negatives_filtered[features]\n",
    "            if 0 in negatives_filtered['evaluation_label'].tolist():\n",
    "                continue\n",
    "            if negatives_filtered.empty:\n",
    "                continue\n",
    "            neg_list = []\n",
    "            if num_rows < len(negatives_filtered):\n",
    "                negatives_filtered = negatives_filtered[negatives_filtered['evaluation_label'] == -1]\n",
    "#                 top_sample_df = negatives_filtered.sample(n=(num_rows - sum([len(x) for x in neg_list])))\n",
    "                \n",
    "                remaining_negatives = negatives_filtered\n",
    "                for feature in train_features:\n",
    "                    sorted_by_feature_df = remaining_negatives.sort_values(feature,ascending=False)\n",
    "                    neg_list.append(sorted_by_feature_df[:2])\n",
    "                    remaining_negatives = sorted_by_feature_df[2:]\n",
    "                top_sample_df = remaining_negatives.sample(n=(num_rows - sum([len(x) for x in neg_list])))\n",
    "    \n",
    "#                 retrieval_score_df = negatives_filtered.sort_values('retrieval_score',ascending=False)\n",
    "#                 neg_list.append(retrieval_score_df[:2])\n",
    "#                 pagerank_score_df = retrieval_score_df[2:].sort_values('pagerank', ascending=False)\n",
    "#                 neg_list.append(pagerank_score_df[:2])\n",
    "                \n",
    "# #                 class_count_score_df = pagerank_score_df[2:].sort_values('lof_class_count_tf_idf_score', ascending=False)\n",
    "# #                 neg_list.append(class_count_score_df[:2])\n",
    "# #                 prop_count_score_df = class_count_score_df[2:].sort_values('lof_property_count_tf_idf_score', ascending=False)\n",
    "# #                 neg_list.append(prop_count_score_df[:2])\n",
    "                \n",
    "#                 monge_elkan_score_df = pagerank_score_df[2:].sort_values('monge_elkan', ascending=False)\n",
    "#                 neg_list.append(monge_elkan_score_df[:2])\n",
    "#                 jaro_winkler_score_df = monge_elkan_score_df[2:].sort_values('jaro_winkler', ascending=False)\n",
    "#                 neg_list.append(jaro_winkler_score_df[:2])\n",
    "# #                 top_sample_df = jaro_winkler_score_df.sample(n=(num_rows - sum([len(x) for x in neg_list])))\n",
    "                \n",
    "# #                 graph_embedding_score_df = jaro_winkler_score_df[2:].sort_values('lof-graph-embedding-score', ascending=False)\n",
    "# #                 neg_list.append(graph_embedding_score_df[:2])\n",
    "# #                 top_sample_df = graph_embedding_score_df.sample(n=(num_rows - sum([len(x) for x in neg_list])))\n",
    "                \n",
    "# #                 h_embedding_score_df = jaro_winkler_score_df[2:].sort_values('lof-H_5x8-embedding-score', ascending=False)\n",
    "# #                 neg_list.append(h_embedding_score_df[:2])\n",
    "# #                 a_embedding_score_df = h_embedding_score_df[2:].sort_values('lof-A-embedding-score', ascending=False)\n",
    "# #                 neg_list.append(a_embedding_score_df[:2])\n",
    "# #                 s_embedding_score_df = a_embedding_score_df[2:].sort_values('lof-S-embedding-score', ascending=False)\n",
    "# #                 neg_list.append(s_embedding_score_df[:2])\n",
    "# #                 top_sample_df = s_embedding_score_df.sample(n=(num_rows - sum([len(x) for x in neg_list])))\n",
    "\n",
    "# #                 eval_label_df = negatives_filtered.sort_values('evaluation_label_copy', ascending=False)\n",
    "# #                 neg_list.append(eval_label_df[:2])\n",
    "# #                 top_sample_df = eval_label_df.sample(n=(num_rows - sum([len(x) for x in neg_list])))\n",
    "        \n",
    "# #                 profile_complex_df = eval_label_df[2:].sort_values('lof-Profile-ComplEx-embedding-score', ascending=False)\n",
    "# #                 neg_list.append(profile_complex_df[:2])\n",
    "# #                 top_sample_df = profile_complex_df.sample(n=(num_rows - sum([len(x) for x in neg_list])))\n",
    "\n",
    "#                 profile_complex_df = jaro_winkler_score_df[2:].sort_values('Profile-ComplEx-embedding-score', ascending=False)\n",
    "#                 neg_list.append(profile_complex_df[:2])\n",
    "#                 top_sample_df = profile_complex_df.sample(n=(num_rows - sum([len(x) for x in neg_list])))\n",
    "            \n",
    "                neg_list.append(top_sample_df)\n",
    "                top_sample_df = pd.concat(neg_list)\n",
    "                top_sample_df.drop('evaluation_label', inplace=True, axis=1)\n",
    "                top_sample_arr = top_sample_df.to_numpy()\n",
    "            \n",
    "            for i in range(num_rows):\n",
    "                neg_features.append(top_sample_arr[i])\n",
    "            random.shuffle(neg_features)\n",
    "            for i in range(num_rows):\n",
    "                pos_row_sample = pos_row.sample(n=1)\n",
    "                ar = pos_row_sample.to_numpy()\n",
    "                for ps_ar in ar:\n",
    "                    pos_features.append(ps_ar)\n",
    "            positive_features_final.append(pos_features)\n",
    "            negative_features_final.append(neg_features)\n",
    "    print(len(positive_features_final), len(positive_features_final[37]))\n",
    "    print(len(negative_features_final), len(negative_features_final[37]))\n",
    "    pickle.dump(positive_features_final,open(args.pos_output,'wb'))\n",
    "    pickle.dump(negative_features_final,open(args.neg_output,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "5fc359db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 1/38 [00:01<01:10,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39173938_0_7916056990138658530.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/38 [00:03<00:47,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10579449_0_1681126353774891032.csv\n",
      "21362676_0_6854186738074119688.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 4/38 [00:05<00:51,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38428277_0_1311643810102462607.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 5/38 [00:07<00:54,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91959037_0_7907661684242014480.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 6/38 [00:12<01:25,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20135078_0_7570343137119682530.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 7/38 [00:14<01:15,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35188621_0_6058553107571275232.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 8/38 [00:16<01:10,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54719588_0_8417197176086756912.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▎       | 9/38 [00:22<01:33,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21245481_0_8730460088443117515.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▋       | 10/38 [00:24<01:21,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71840765_0_6664391841933033844.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 11/38 [00:24<00:57,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8468806_0_4382447409703007384.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 12/38 [00:26<00:50,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88523363_0_8180214313099580515.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 13/38 [00:31<01:13,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29414811_13_8724394428539174350.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 14/38 [00:31<00:51,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99070098_0_2074872741302696997.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 15/38 [00:36<01:05,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43237185_1_3636357855502246981.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 16/38 [00:36<00:45,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46671561_0_6122315295162029872.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▍     | 17/38 [00:40<00:54,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25404227_0_2240631045609013057.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 18/38 [00:42<00:52,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9834884_0_3871985887467090123.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 19/38 [00:49<01:15,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63450419_0_8012592961815711786.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 20/38 [00:54<01:12,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22864497_0_8632623712684511496.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 21/38 [01:01<01:25,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53822652_0_5767892317858575530.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 22/38 [01:09<01:35,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37856682_0_6818907050314633217.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 23/38 [01:14<01:25,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29414811_12_251152470253168163.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 24/38 [01:15<00:57,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69537082_0_7789694313271016902.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 25/38 [01:19<00:53,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60319454_0_3938426910282115527.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 26/38 [01:20<00:37,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767252_0_2409448375013995751.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 27/38 [01:21<00:30,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84548468_0_5955155464119382182.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▎  | 28/38 [01:24<00:26,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80588006_0_6965325215443683359.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▋  | 29/38 [01:24<00:17,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39650055_5_7135804139753401681.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 30/38 [01:27<00:17,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40534006_0_4617468856744635526.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 31/38 [01:28<00:12,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90196673_0_5458330029110291950.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 32/38 [01:37<00:24,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24036779_0_5608105867560183058.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 34/38 [01:39<00:09,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9567241_0_5666388268510912770.csv\n",
      "41480166_0_6681239260286218499.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 35/38 [01:42<00:08,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77694908_0_6083291340991074532.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▍| 36/38 [01:44<00:05,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39107734_2_2329160387535788734.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 37/38 [01:45<00:02,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50245608_0_871275842592178099.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [01:49<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3831 64\n",
      "3831 64\n"
     ]
    }
   ],
   "source": [
    "gen_training_data_args = Namespace(train_path=train_path, pos_output=pos_output, neg_output=neg_output, \n",
    "                 min_max_scaler_path=min_max_scaler_path)\n",
    "all_data = merge_files(gen_training_data_args)\n",
    "scaler = compute_normalization_factor(gen_training_data_args, all_data)\n",
    "generate_train_data(gen_training_data_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513dc26",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "2fe30c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class T2DV2Dataset(Dataset):\n",
    "    def __init__(self, pos_features, neg_features):\n",
    "        self.pos_features = pos_features\n",
    "        self.neg_features = neg_features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pos_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos_features[idx], self.neg_features[idx]\n",
    "\n",
    "# Model\n",
    "class PairwiseNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        #original 12x24, 24x12, 12x12, 12x1\n",
    "        self.fc1 = nn.Linear(hidden_size, 2*hidden_size)\n",
    "        self.fc2 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, pos_features, neg_features):\n",
    "        # Positive pass\n",
    "        x = F.relu(self.fc1(pos_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pos_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        # Negative Pass\n",
    "        x = F.relu(self.fc1(neg_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        neg_out = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return pos_out, neg_out\n",
    "    \n",
    "    def predict(self, test_feat):\n",
    "        x = F.relu(self.fc1(test_feat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        test_out = torch.sigmoid(self.fc4(x))\n",
    "        return test_out\n",
    "\n",
    "# Pairwise Loss\n",
    "class PairwiseLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = 0\n",
    "    \n",
    "    def forward(self, pos_out, neg_out):\n",
    "        distance = (1 - pos_out) + neg_out\n",
    "        loss = torch.mean(torch.max(torch.tensor(0), distance))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe782f8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "a7560790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(positive_feat_path, negative_feat_path):\n",
    "    pos_features = pickle.load(open(positive_feat_path, 'rb'))\n",
    "    neg_features = pickle.load(open(negative_feat_path, 'rb'))\n",
    "\n",
    "    pos_features_flatten = list(chain.from_iterable(pos_features))\n",
    "    neg_features_flatten = list(chain.from_iterable(neg_features))\n",
    "\n",
    "    train_dataset = T2DV2Dataset(pos_features_flatten, neg_features_flatten)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "    return train_dataloader\n",
    "\n",
    "def infer_scores(min_max_scaler_path, input_table_path, output_table_path, model):\n",
    "    scaler = pickle.load(open(min_max_scaler_path, 'rb'))\n",
    "    \n",
    "    normalize_features = train_features\n",
    "    \n",
    "    for file in glob.glob(input_table_path + '*.csv'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        if os.path.getsize(file) == 0:\n",
    "            continue\n",
    "        if file_name == '52299421_0_4473286348258170200.csv':\n",
    "            continue\n",
    "        print(file_name)\n",
    "        d_sample = pd.read_csv(file)\n",
    "        grouped_obj = d_sample.groupby(['column', 'row'])\n",
    "        new_df_list = []\n",
    "        pred = []\n",
    "        for cell in grouped_obj:\n",
    "            cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "\n",
    "            #TODO -- FIGURE OUT HOW TO MAKE THIS CODE NOT DEPEND ON THE CHOSEN FEATURES -- \n",
    "            #        CHANGED FROM GRAPH EMBEDDING TO RETRIEVAL SCORE\n",
    "\n",
    "#             sorted_df = cell[1].sort_values('retrieval_score',ascending=False)\n",
    "#             sorted_df = cell[1].sort_values('levenshtein',ascending=False)\n",
    "#             sorted_df = cell[1].sort_values('Profile-ComplEx-embedding-score',ascending=False)\n",
    "            sorted_df = cell[1]\n",
    "            sorted_df_features = sorted_df[normalize_features]\n",
    "            new_df_list.append(sorted_df)\n",
    "            arr = sorted_df_features.to_numpy()\n",
    "            test_inp = []\n",
    "            for a in arr:\n",
    "                test_inp.append(a)\n",
    "            test_tensor = torch.tensor(test_inp).float()\n",
    "            scores = model.predict(test_tensor)\n",
    "            # new code\n",
    "            scores_list = torch.squeeze(scores).tolist()\n",
    "            if not type(scores_list) is list:\n",
    "                pred.append(scores_list)\n",
    "            else:\n",
    "                pred.extend(scores_list)\n",
    "            #---OLD CODE---\n",
    "#             pred.extend(torch.squeeze(scores).tolist())\n",
    "        test_df = pd.concat(new_df_list)\n",
    "        test_df['siamese_pred'] = pred\n",
    "        test_df.to_csv(os.path.join(output_table_path, file_name), index=False)\n",
    "\n",
    "def train(args):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    train_dataloader = generate_dataloader(args.positive_feat_path, args.negative_feat_path)\n",
    "    criterion = PairwiseLoss()\n",
    "    EPOCHS = args.num_epochs\n",
    "    model = PairwiseNetwork(len(train_features)).to(device=device)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    top1_max_prec = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        model.train()\n",
    "        for bid, batch in tqdm(enumerate(train_dataloader), position=0, leave=True):\n",
    "            positive_feat = torch.tensor(batch[0].float())\n",
    "            negative_feat = torch.tensor(batch[1].float())\n",
    "            optimizer.zero_grad()\n",
    "            pos_out, neg_out = model(positive_feat, negative_feat)\n",
    "            loss = criterion(pos_out, neg_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss += loss\n",
    "        avg_loss = train_epoch_loss / bid\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        infer_scores(args.min_max_scaler_path, args.dev_path, args.dev_output, model)\n",
    "        eval_data = merge_eval_files(args.dev_output)\n",
    "        res, candidate_eval_data = parse_eval_files_stats(eval_data, 'siamese_pred')\n",
    "        top1_precision = res['expected_num_tasks_with_model_score_top_one_accurate']/res['num_tasks_with_gt']\n",
    "        if top1_precision > top1_max_prec:\n",
    "            top1_max_prec = top1_precision\n",
    "            model_save_name = 'epoch_{}_loss_{}_top1_{}.pth'.format(epoch, avg_loss, top1_max_prec)\n",
    "            best_model_path = os.path.join(args.model_save_path, model_save_name)\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        print(\"Epoch {}, Avg Loss is {}, epoch top1 {}, max top1 {}\".format(epoch, avg_loss, top1_precision, top1_max_prec))\n",
    "        \n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "3fc62681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval_files(final_score_path):\n",
    "    eval_file_names = []\n",
    "    df_list = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(final_score_path):\n",
    "        for fn in filenames:\n",
    "            if fn != '52299421_0_4473286348258170200.csv':\n",
    "                if \"csv\" not in fn:\n",
    "                    continue\n",
    "                abs_fn = os.path.join(dirpath, fn)\n",
    "                assert os.path.isfile(abs_fn)\n",
    "                if os.path.getsize(abs_fn) == 0:\n",
    "                    continue\n",
    "                eval_file_names.append(abs_fn)\n",
    "    \n",
    "    for fn in eval_file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        # df = df.fillna('')\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def parse_eval_files_stats(eval_data, method):\n",
    "    table_rows_w_gt_max = {}\n",
    "    \n",
    "    res = {}\n",
    "    candidate_eval_data = eval_data.groupby(['table_id', 'row', 'column'])['table_id'].count().reset_index(name=\"count\")\n",
    "    res['num_tasks'] = len(eval_data.groupby(['table_id', 'row', 'column']))\n",
    "    res['num_tasks_with_gt'] = len(eval_data[pd.notna(eval_data['GT_kg_id'])].groupby(['table_id', 'row', 'column']))\n",
    "    res['num_tasks_with_gt_in_candidate'] = len(eval_data[eval_data['evaluation_label'] == 1].groupby(['table_id', 'row', 'column']))\n",
    "    res['num_tasks_with_singleton_candidate'] = len(candidate_eval_data[candidate_eval_data['count'] == 1].groupby(['table_id', 'row', 'column']))\n",
    "    singleton_eval_data = candidate_eval_data[candidate_eval_data['count'] == 1]\n",
    "    num_tasks_with_singleton_candidate_with_gt = 0\n",
    "    for i, row in singleton_eval_data.iterrows():\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) == 1\n",
    "        if c_e_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_singleton_candidate_with_gt += 1\n",
    "    res['num_tasks_with_singleton_candidate_with_gt'] = num_tasks_with_singleton_candidate_with_gt\n",
    "    num_tasks_with_graph_top_one_accurate = []\n",
    "    num_tasks_with_graph_top_five_accurate = []\n",
    "    num_tasks_with_graph_top_ten_accurate = []\n",
    "    expected_num_tasks_with_model_score_top_one_accurate = []\n",
    "    num_tasks_with_model_score_top_one_accurate = []\n",
    "    num_tasks_with_model_score_top_five_accurate = []\n",
    "    num_tasks_with_model_score_top_ten_accurate = []\n",
    "    has_gt_list = []\n",
    "    has_gt_in_candidate = []\n",
    "    # candidate_eval_data = candidate_eval_data[:1]\n",
    "    for i, row in candidate_eval_data.iterrows():\n",
    "        #print(i)\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        \n",
    "        if table_id not in table_rows_w_gt_max:\n",
    "            table_rows_w_gt_max[table_id] = {}\n",
    "        if row_idx not in table_rows_w_gt_max[table_id]:\n",
    "            table_rows_w_gt_max[table_id][row_idx] = 0\n",
    "        \n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) > 0\n",
    "        if np.nan not in set(c_e_data['GT_kg_id']):\n",
    "            has_gt_list.append(1)\n",
    "        else:\n",
    "            has_gt_list.append(0)\n",
    "        if 1 in set(c_e_data['evaluation_label']):\n",
    "            has_gt_in_candidate.append(1)\n",
    "        else:\n",
    "            has_gt_in_candidate.append(0)\n",
    "            \n",
    "        # handle graph-embedding-score\n",
    "#         s_data = c_e_data.sort_values(by=['lof-graph-embedding-score'], ascending=False)\n",
    "#         if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "#             num_tasks_with_graph_top_one_accurate.append(1)\n",
    "#         else:\n",
    "#             num_tasks_with_graph_top_one_accurate.append(0)\n",
    "#         if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "#             num_tasks_with_graph_top_five_accurate.append(1)\n",
    "#         else:\n",
    "#             num_tasks_with_graph_top_five_accurate.append(0)\n",
    "#         if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "#             num_tasks_with_graph_top_ten_accurate.append(1)\n",
    "#         else:\n",
    "#             num_tasks_with_graph_top_ten_accurate.append(0)\n",
    "        \n",
    "        #rank on model score\n",
    "\n",
    "        cands_w_max_score = c_e_data.loc[c_e_data.loc[:,method] == max(c_e_data.loc[:,method])]\n",
    "        if 1 in cands_w_max_score.loc[:,\"evaluation_label\"].values:\n",
    "            table_rows_w_gt_max[table_id][row_idx] = 1\n",
    "            expected_num_tasks_with_model_score_top_one_accurate.append(1 / len(cands_w_max_score.loc[:,\"kg_id\"].unique()))\n",
    "            \n",
    "        s_data = c_e_data.sort_values(by=[method], ascending=False) \n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(0)\n",
    "            \n",
    "#         cf_e_data = c_e_data.copy()\n",
    "#         cf_e_data['lof-graph-embedding-score'] = cf_e_data['lof-graph-embedding-score'].replace(np.nan, 0)\n",
    "#         cf_e_data[method] = cf_e_data[method].replace(np.nan, 0)\n",
    "\n",
    "#     candidate_eval_data['lof-graph_top_one_accurate'] = num_tasks_with_graph_top_one_accurate\n",
    "#     candidate_eval_data['lof-graph_top_five_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "#     candidate_eval_data['lof-graph_top_ten_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['model_top_one_accurate'] = num_tasks_with_model_score_top_one_accurate\n",
    "    candidate_eval_data['model_top_five_accurate'] = num_tasks_with_model_score_top_five_accurate\n",
    "    candidate_eval_data['model_top_ten_accurate'] = num_tasks_with_model_score_top_ten_accurate\n",
    "    candidate_eval_data['has_gt'] = has_gt_list\n",
    "    candidate_eval_data['has_gt_in_candidate'] = has_gt_in_candidate\n",
    "#     res['num_tasks_with_graph_top_one_accurate'] = sum(num_tasks_with_graph_top_one_accurate)\n",
    "#     res['num_tasks_with_graph_top_five_accurate'] = sum(num_tasks_with_graph_top_five_accurate)\n",
    "#     res['num_tasks_with_graph_top_ten_accurate'] = sum(num_tasks_with_graph_top_ten_accurate)\n",
    "    res['num_tasks_with_model_score_top_one_accurate'] = sum(num_tasks_with_model_score_top_one_accurate)\n",
    "    res['num_tasks_with_model_score_top_five_accurate'] = sum(num_tasks_with_model_score_top_five_accurate)\n",
    "    res['num_tasks_with_model_score_top_ten_accurate'] = sum(num_tasks_with_model_score_top_ten_accurate)\n",
    "    res['expected_num_tasks_with_model_score_top_one_accurate'] = sum(expected_num_tasks_with_model_score_top_one_accurate)\n",
    "#     print(expected_num_tasks_with_model_score_top_one_accurate)\n",
    "    return res, candidate_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "261babb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Namespace(num_epochs=20, lr=0.001, positive_feat_path=pos_output, negative_feat_path=neg_output,\n",
    "                         dev_path=dev_path, dev_output=dev_output_predictions,\n",
    "                         model_save_path=model_save_path, min_max_scaler_path=min_max_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "fe95be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "a80d79d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:08, 469.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:00, 487.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Loss is 0.36663055419921875, epoch top1 0.5836614173228346, max top1 0.5836614173228346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:11, 328.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:00, 485.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss is 0.2597721517086029, epoch top1 0.6145508225129447, max top1 0.6145508225129447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:08, 473.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106it [00:00, 524.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss is 0.25142693519592285, epoch top1 0.6282013041609499, max top1 0.6282013041609499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 513.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [00:00, 515.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss is 0.24671632051467896, epoch top1 0.6380539788266166, max top1 0.6380539788266166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 516.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 494.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss is 0.24150140583515167, epoch top1 0.6420870348424641, max top1 0.6420870348424641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 506.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 498.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss is 0.23349884152412415, epoch top1 0.6481988044734107, max top1 0.6481988044734107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 514.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 513.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss is 0.2299794703722, epoch top1 0.655639280663887, max top1 0.655639280663887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 517.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 492.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss is 0.22154106199741364, epoch top1 0.6622070678665166, max top1 0.6622070678665166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 498.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [00:00, 510.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss is 0.21871298551559448, epoch top1 0.6669252280964879, max top1 0.6669252280964879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 506.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [00:00, 452.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss is 0.2180570662021637, epoch top1 0.6691794207542238, max top1 0.6691794207542238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 505.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 498.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss is 0.2180592119693756, epoch top1 0.670064395359671, max top1 0.670064395359671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 507.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 498.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss is 0.21768595278263092, epoch top1 0.6703701099862517, max top1 0.6703701099862517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 507.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 383.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss is 0.2167365998029709, epoch top1 0.6644977751665903, max top1 0.6703701099862517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:08, 469.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 437.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss is 0.21559661626815796, epoch top1 0.661651052866604, max top1 0.6703701099862517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 497.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:00, 510.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss is 0.21498870849609375, epoch top1 0.6613950380087349, max top1 0.6703701099862517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 494.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:00, 481.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss is 0.21107600629329681, epoch top1 0.6601789933236784, max top1 0.6703701099862517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 508.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [00:00, 469.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss is 0.21057838201522827, epoch top1 0.6627276839279951, max top1 0.6703701099862517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 506.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 493.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss is 0.2127298265695572, epoch top1 0.6678442318595035, max top1 0.6703701099862517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 488.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 493.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss is 0.21676358580589294, epoch top1 0.662149631184588, max top1 0.6703701099862517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [00:07, 511.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n",
      "Epoch 19, Avg Loss is 0.2180793732404709, epoch top1 0.6563080672578704, max top1 0.6703701099862517\n"
     ]
    }
   ],
   "source": [
    "## Call Training\n",
    "best_model_path = train(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb498e",
   "metadata": {},
   "source": [
    "## Computing top1 accuracy on dev set with custom code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "a93bc321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 96\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:51 epoch_0_loss_0.36663055419921875_top1_0.5836614173228346.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:54 epoch_10_loss_0.2180592119693756_top1_0.670064395359671.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:55 epoch_11_loss_0.21768595278263092_top1_0.6703701099862517.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:52 epoch_1_loss_0.2597721517086029_top1_0.6145508225129447.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:52 epoch_2_loss_0.25142693519592285_top1_0.6282013041609499.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:52 epoch_3_loss_0.24671632051467896_top1_0.6380539788266166.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:53 epoch_4_loss_0.24150140583515167_top1_0.6420870348424641.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:53 epoch_5_loss_0.23349884152412415_top1_0.6481988044734107.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:53 epoch_6_loss_0.2299794703722_top1_0.655639280663887.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:53 epoch_7_loss_0.22154106199741364_top1_0.6622070678665166.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:54 epoch_8_loss_0.21871298551559448_top1_0.6669252280964879.pth\r\n",
      "-rw-r--r--  1 nicklein  staff   3.3K Sep 10 15:54 epoch_9_loss_0.2180570662021637_top1_0.6691794207542238.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {model_save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "0ecc1f5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    }
   ],
   "source": [
    "# model = TheModelClass(*args, **kwargs)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = PairwiseNetwork(len(train_features)).to(device=device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"{}\".format(best_model_path)))\n",
    "model.eval()\n",
    "infer_scores(min_max_scaler_path, dev_path, dev_output_predictions, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "9db20a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:19<00:00,  2.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>top1 acc in attainable cells</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Countries II(35)</td>\n",
       "      <td>0.3313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Companies(13)</td>\n",
       "      <td>0.7308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Religious People(151)</td>\n",
       "      <td>0.6540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Video Games(16)</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Movies(75)</td>\n",
       "      <td>0.4849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Baseball Players I(20)</td>\n",
       "      <td>0.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Baseball Players II(126)</td>\n",
       "      <td>0.8320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Journals/Magazines(72)</td>\n",
       "      <td>0.7986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Macro Avg</td>\n",
       "      <td>0.6321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Micro Avg</td>\n",
       "      <td>0.6704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      table top1 acc in attainable cells\n",
       "0          Countries II(35)                       0.3313\n",
       "1             Companies(13)                       0.7308\n",
       "2     Religious People(151)                       0.6540\n",
       "3           Video Games(16)                       0.6250\n",
       "4                Movies(75)                       0.4849\n",
       "5    Baseball Players I(20)                       0.6000\n",
       "6  Baseball Players II(126)                       0.8320\n",
       "7    Journals/Magazines(72)                       0.7986\n",
       "8                 Macro Avg                       0.6321\n",
       "9                 Micro Avg                       0.6704"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tabulate import tabulate\n",
    "def choose_best_candidates_from_scores(scores):\n",
    "    choices = []\n",
    "    for cell_candidate_scores in scores:\n",
    "        # we need to break ties randomly since the input data may not be formatted randomly\n",
    "        # This is specifically the case with the 'chiefs' data - first row of each cell is the GT\n",
    "        max_score = max(cell_candidate_scores.values())\n",
    "        choices.append(random.choice([candidate for candidate, score in cell_candidate_scores.items() if score == max_score]))\n",
    "    return choices\n",
    "\n",
    "\n",
    "num_trials = 10\n",
    "\n",
    "headers = [\"table\", \"top1 acc in attainable cells\"]#,\"cols\"]#, \"top1 acc\", \"top 1 acc ignoring ties\"]\n",
    "rows = []\n",
    "micro_avg_acc = 0\n",
    "total_num_cells = 0\n",
    "\n",
    "nic_table_rows_w_gt_max={}\n",
    "\n",
    "for file in tqdm(sorted(glob.glob(f'{working_path}/dev_predictions/*.csv'))):\n",
    "# file = \"/data02/profiling/contrastive_loss_nn_w_embedding_scores/dev_predictions/14067031_0_559833072073397908.csv\"\n",
    "\n",
    "    file_name = file.split('/')[-1]\n",
    "    \n",
    "#     if file_name != 'MOX6MBH5.csv':\n",
    "#         continue\n",
    "    \n",
    "    nic_table_rows_w_gt_max[file_name] = {}\n",
    "    \n",
    "    df = pd.read_csv(file).fillna(0)\n",
    "\n",
    "    scores = df.groupby(['column','row']).apply(lambda g: {cand : g.loc[g.loc[:,\"kg_id\"]==cand,\"siamese_pred\"].max() for cand in g.loc[:,\"kg_id\"].unique()}).to_list()\n",
    "    cells = df.groupby(['column','row'])[\"kg_id\"].apply(list).to_list()\n",
    "    cells_gt = [l[0] for l in df.groupby(['column','row'])[\"GT_kg_id\"].apply(list).to_list()]\n",
    "    cell_idx_to_col = [l[0] for l in df.groupby(['column','row'])[\"column\"].apply(list).to_list()]\n",
    "    cell_idx_to_row = [l[0] for l in df.groupby(['column','row'])[\"row\"].apply(list).to_list()]\n",
    "\n",
    "#     avg_f1=0\n",
    "#     for i in range(num_trials):\n",
    "#         choices = choose_best_candidates_from_scores(scores)\n",
    "#         avg_f1 += f1_score(choices, cells_gt, average=\"micro\")\n",
    "#     avg_f1 /= num_trials\n",
    "    \n",
    "    top1_acc = 0\n",
    "    top1_acc_ignore_ties = 0\n",
    "    num_no_ties_correct = 0\n",
    "    num_no_ties = 0\n",
    "    num_attainable_cells = 0\n",
    "    l = []\n",
    "    for cell_idx in range(len(cells_gt)):\n",
    "        \n",
    "        row_num = cell_idx_to_row[cell_idx]\n",
    "        col_num = cell_idx_to_col[cell_idx]\n",
    "#         if not (row_num ==3 and col_num ==0):\n",
    "#             continue\n",
    "        if col_num not in nic_table_rows_w_gt_max[file_name]:\n",
    "            nic_table_rows_w_gt_max[file_name][col_num] = {}\n",
    "        nic_table_rows_w_gt_max[file_name][col_num][row_num] = 0\n",
    "        \n",
    "        gt = cells_gt[cell_idx]\n",
    "#         print(gt)\n",
    "#         print(gt in scores[cell_idx])\n",
    "#         if gt in scores[cell_idx]:\n",
    "#             print(scores[cell_idx][gt])\n",
    "#             print(max(scores[cell_idx].values()))\n",
    "        max_score_in_row = max(scores[cell_idx].values())\n",
    "        if gt in [cand for cand, score in scores[cell_idx].items() if score == max_score_in_row]:\n",
    "            \n",
    "            nic_table_rows_w_gt_max[file_name][col_num][row_num] = 1\n",
    "            \n",
    "            num_cands_tied = len([1 for val in scores[cell_idx].values() if val==max(scores[cell_idx].values())])\n",
    "            l.append(1 / num_cands_tied)\n",
    "            top1_acc += (1 / num_cands_tied)\n",
    "            top1_acc_ignore_ties += 1\n",
    "        if gt in cells[cell_idx]:\n",
    "            num_attainable_cells += 1\n",
    "    micro_avg_acc += top1_acc\n",
    "    total_num_cells += len(cells_gt)\n",
    "    top1_attainable_acc = top1_acc / num_attainable_cells\n",
    "    top1_acc /= len(cells_gt)\n",
    "    top1_acc_ignore_ties /= len(cells_gt)\n",
    "#     print(l)\n",
    "#     print(len(l))\n",
    "    \n",
    "    row = [file_name + f\"({len(cells_gt)})\", top1_acc]\n",
    "    rows.append(row)\n",
    "    \n",
    "\n",
    "avg_row_scores = [0]\n",
    "for row in rows:\n",
    "    for i in range(len(avg_row_scores)):\n",
    "        avg_row_scores[i] += row[i+1]\n",
    "avg_row_scores = [score / len(rows) for score in avg_row_scores]\n",
    "rows.append([\"Macro Avg\"] + avg_row_scores)\n",
    "rows.append([\"Micro Avg\"] + [micro_avg_acc / total_num_cells])\n",
    "\n",
    "rows = [[\"{:.4f}\".format(row[i]) if i > 0 else row[i] for i in range(len(row))] for row in rows]\n",
    "\n",
    "results_df = pd.DataFrame(data=rows, columns=headers)\n",
    "display(results_df)\n",
    "results_df.to_csv(f\"{working_path}/best_dev_set_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "a25db494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro: 0.6704\n",
      "macro: 0.6321\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv(f\"{working_path}/best_dev_set_results.csv\")\n",
    "macro = results_df.loc[results_df.loc[:,\"table\"]==\"Macro Avg\",\"top1 acc in attainable cells\"].item()\n",
    "micro = results_df.loc[results_df.loc[:,\"table\"]==\"Micro Avg\",\"top1 acc in attainable cells\"].item()\n",
    "print(f\"micro: {micro}\\nmacro: {macro}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04e54a",
   "metadata": {},
   "source": [
    "## Computing top1 accuracy on test set with custom code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "9e932a6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    }
   ],
   "source": [
    "# model = TheModelClass(*args, **kwargs)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = PairwiseNetwork(len(train_features)).to(device=device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"{}\".format(best_model_path)))\n",
    "model.eval()\n",
    "infer_scores(min_max_scaler_path, test_path, test_output_predictions, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "08cce97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:19<00:00,  2.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>top1 acc in attainable cells</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Countries II(35)</td>\n",
       "      <td>0.3313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Companies(13)</td>\n",
       "      <td>0.7308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Religious People(151)</td>\n",
       "      <td>0.6540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Video Games(16)</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Movies(75)</td>\n",
       "      <td>0.4849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Baseball Players I(20)</td>\n",
       "      <td>0.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Baseball Players II(126)</td>\n",
       "      <td>0.8320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Journals/Magazines(72)</td>\n",
       "      <td>0.7986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Macro Avg</td>\n",
       "      <td>0.6321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Micro Avg</td>\n",
       "      <td>0.6704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      table top1 acc in attainable cells\n",
       "0          Countries II(35)                       0.3313\n",
       "1             Companies(13)                       0.7308\n",
       "2     Religious People(151)                       0.6540\n",
       "3           Video Games(16)                       0.6250\n",
       "4                Movies(75)                       0.4849\n",
       "5    Baseball Players I(20)                       0.6000\n",
       "6  Baseball Players II(126)                       0.8320\n",
       "7    Journals/Magazines(72)                       0.7986\n",
       "8                 Macro Avg                       0.6321\n",
       "9                 Micro Avg                       0.6704"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tabulate import tabulate\n",
    "def choose_best_candidates_from_scores(scores):\n",
    "    choices = []\n",
    "    for cell_candidate_scores in scores:\n",
    "        # we need to break ties randomly since the input data may not be formatted randomly\n",
    "        # This is specifically the case with the 'chiefs' data - first row of each cell is the GT\n",
    "        max_score = max(cell_candidate_scores.values())\n",
    "        choices.append(random.choice([candidate for candidate, score in cell_candidate_scores.items() if score == max_score]))\n",
    "    return choices\n",
    "\n",
    "\n",
    "num_trials = 10\n",
    "\n",
    "headers = [\"table\", \"top1 acc in attainable cells\"]#,\"cols\"]#, \"top1 acc\", \"top 1 acc ignoring ties\"]\n",
    "rows = []\n",
    "micro_avg_acc = 0\n",
    "total_num_cells = 0\n",
    "\n",
    "nic_table_rows_w_gt_max={}\n",
    "\n",
    "for file in tqdm(sorted(glob.glob(f'{working_path}/test_predictions/*.csv'))):\n",
    "# file = \"/data02/profiling/contrastive_loss_nn_w_embedding_scores/dev_predictions/14067031_0_559833072073397908.csv\"\n",
    "\n",
    "    file_name = file.split('/')[-1]\n",
    "    \n",
    "#     if file_name != 'MOX6MBH5.csv':\n",
    "#         continue\n",
    "    \n",
    "    nic_table_rows_w_gt_max[file_name] = {}\n",
    "    \n",
    "    df = pd.read_csv(file).fillna(0)\n",
    "\n",
    "    scores = df.groupby(['column','row']).apply(lambda g: {cand : g.loc[g.loc[:,\"kg_id\"]==cand,\"siamese_pred\"].max() for cand in g.loc[:,\"kg_id\"].unique()}).to_list()\n",
    "    cells = df.groupby(['column','row'])[\"kg_id\"].apply(list).to_list()\n",
    "    cells_gt = [l[0] for l in df.groupby(['column','row'])[\"GT_kg_id\"].apply(list).to_list()]\n",
    "    cell_idx_to_col = [l[0] for l in df.groupby(['column','row'])[\"column\"].apply(list).to_list()]\n",
    "    cell_idx_to_row = [l[0] for l in df.groupby(['column','row'])[\"row\"].apply(list).to_list()]\n",
    "\n",
    "#     avg_f1=0\n",
    "#     for i in range(num_trials):\n",
    "#         choices = choose_best_candidates_from_scores(scores)\n",
    "#         avg_f1 += f1_score(choices, cells_gt, average=\"micro\")\n",
    "#     avg_f1 /= num_trials\n",
    "    \n",
    "    top1_acc = 0\n",
    "    top1_acc_ignore_ties = 0\n",
    "    num_no_ties_correct = 0\n",
    "    num_no_ties = 0\n",
    "    num_attainable_cells = 0\n",
    "    l = []\n",
    "    for cell_idx in range(len(cells_gt)):\n",
    "        \n",
    "        row_num = cell_idx_to_row[cell_idx]\n",
    "        col_num = cell_idx_to_col[cell_idx]\n",
    "#         if not (row_num ==3 and col_num ==0):\n",
    "#             continue\n",
    "        if col_num not in nic_table_rows_w_gt_max[file_name]:\n",
    "            nic_table_rows_w_gt_max[file_name][col_num] = {}\n",
    "        nic_table_rows_w_gt_max[file_name][col_num][row_num] = 0\n",
    "        \n",
    "        gt = cells_gt[cell_idx]\n",
    "#         print(gt)\n",
    "#         print(gt in scores[cell_idx])\n",
    "#         if gt in scores[cell_idx]:\n",
    "#             print(scores[cell_idx][gt])\n",
    "#             print(max(scores[cell_idx].values()))\n",
    "        max_score_in_row = max(scores[cell_idx].values())\n",
    "        if gt in [cand for cand, score in scores[cell_idx].items() if score == max_score_in_row]:\n",
    "            \n",
    "            nic_table_rows_w_gt_max[file_name][col_num][row_num] = 1\n",
    "            \n",
    "            num_cands_tied = len([1 for val in scores[cell_idx].values() if val==max(scores[cell_idx].values())])\n",
    "            l.append(1 / num_cands_tied)\n",
    "            top1_acc += (1 / num_cands_tied)\n",
    "            top1_acc_ignore_ties += 1\n",
    "        if gt in cells[cell_idx]:\n",
    "            num_attainable_cells += 1\n",
    "    micro_avg_acc += top1_acc\n",
    "    total_num_cells += len(cells_gt)\n",
    "    top1_attainable_acc = top1_acc / num_attainable_cells\n",
    "    top1_acc /= len(cells_gt)\n",
    "    top1_acc_ignore_ties /= len(cells_gt)\n",
    "#     print(l)\n",
    "#     print(len(l))\n",
    "    \n",
    "    row = [file_name + f\"({len(cells_gt)})\", top1_acc]\n",
    "    rows.append(row)\n",
    "    \n",
    "\n",
    "avg_row_scores = [0]\n",
    "for row in rows:\n",
    "    for i in range(len(avg_row_scores)):\n",
    "        avg_row_scores[i] += row[i+1]\n",
    "avg_row_scores = [score / len(rows) for score in avg_row_scores]\n",
    "rows.append([\"Macro Avg\"] + avg_row_scores)\n",
    "rows.append([\"Micro Avg\"] + [micro_avg_acc / total_num_cells])\n",
    "\n",
    "rows = [[\"{:.4f}\".format(row[i]) if i > 0 else row[i] for i in range(len(row))] for row in rows]\n",
    "\n",
    "results_df = pd.DataFrame(data=rows, columns=headers)\n",
    "display(results_df)\n",
    "results_df.to_csv(f\"{working_path}/test_set_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "611f0863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro: 0.6704\n",
      "macro: 0.6321\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv(f\"{working_path}/test_set_results.csv\")\n",
    "macro = results_df.loc[results_df.loc[:,\"table\"]==\"Macro Avg\",\"top1 acc in attainable cells\"].item()\n",
    "micro = results_df.loc[results_df.loc[:,\"table\"]==\"Micro Avg\",\"top1 acc in attainable cells\"].item()\n",
    "print(f\"micro: {micro}\\nmacro: {macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5405b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6eb88562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'14067031_0_559833072073397908': 31,\n",
       " '14380604_4_3329235705746762392': 11,\n",
       " '28086084_0_3127660530989916727': 102,\n",
       " '29414811_2_4773219892816395776': 14,\n",
       " '39759273_0_1427898308030295194': 68,\n",
       " '45073662_0_3179937335063201739': 14,\n",
       " '50270082_0_444360818941411589': 113,\n",
       " '84575189_0_6365692015941409487': 65}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {table_id : sum(rows_correct.values()) for table_id, rows_correct in table_rows_w_gt_max.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4299ff11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'14067031_0_559833072073397908.csv': 31,\n",
       " '14380604_4_3329235705746762392.csv': 11,\n",
       " '28086084_0_3127660530989916727.csv': 102,\n",
       " '29414811_2_4773219892816395776.csv': 14,\n",
       " '39759273_0_1427898308030295194.csv': 68,\n",
       " '45073662_0_3179937335063201739.csv': 14,\n",
       " '50270082_0_444360818941411589.csv': 113,\n",
       " '84575189_0_6365692015941409487.csv': 65}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {table_id : sum(rows_correct.values()) for table_id, rows_correct in nic_table_rows_w_gt_max.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "337369b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1,\n",
       " 2: 1,\n",
       " 3: 1,\n",
       " 4: 1,\n",
       " 5: 1,\n",
       " 7: 1,\n",
       " 8: 0,\n",
       " 9: 1,\n",
       " 10: 0,\n",
       " 12: 0,\n",
       " 13: 1,\n",
       " 14: 0,\n",
       " 17: 1}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nic_table_rows_w_gt_max['14380604_4_3329235705746762392.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "23681a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1,\n",
       " 2: 1,\n",
       " 3: 1,\n",
       " 4: 1,\n",
       " 5: 1,\n",
       " 7: 1,\n",
       " 8: 1,\n",
       " 9: 1,\n",
       " 10: 0,\n",
       " 12: 1,\n",
       " 13: 1,\n",
       " 14: 0,\n",
       " 17: 1}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table_rows_w_gt_max['14380604_4_3329235705746762392']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db847172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(f'{working_path}/dev_predictions/14380604_4_3329235705746762392.csv')\n",
    "# max_val = df.loc[df.loc[:,\"row\"]==8, \"siamese_pred\"].max()\n",
    "# print(max_val)\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "# #     display(df)\n",
    "#     display(df.loc[df.loc[:,\"row\"]==8])\n",
    "# #     display(df.loc[df.loc[:,\"siamese_pred\"]==max_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd26af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
