{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prostate-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arabic-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_url = 'http://ckg07:9200'\n",
    "es_index = 'wikidatadwd-augmented'\n",
    "\n",
    "work_dir = '/Users/amandeep/Github/table-linker/data/t2dv2'\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-train-canonical/\n",
    "train_path = f'{work_dir}/t2dv2-train-canonical/'\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-dev-canonical/\n",
    "dev_path = f'{work_dir}/t2dv2-dev-canonical/'\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-train-candidates-dwd-v2/\n",
    "train_candidate_path = f'{train_path}output-candidates/'\n",
    "train_feature_path = f'{train_path}output-features-2/'\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/t2dv2-dev-candidates-dwd-v2/\n",
    "dev_candidate_path = f'{dev_path}output-candidates/'\n",
    "dev_feature_path = f'{dev_path}output-features-2/'\n",
    "\n",
    "# GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/ground_truth/Xinting_GT_csv\n",
    "ground_truth_files = f'{work_dir}/round_1_GT/'\n",
    "classifier_model_path = '/Users/amandeep/Github/table-linker-pipelines/table-linker-full-pipeline/models/weighted_lr.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cosmetic-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_field = 'graph_embedding_complex,class_count,property_count'\n",
    "temp_dir = f'{work_dir}/temp/' #temp directory to store intermediate files\n",
    "\n",
    "#directory to store the property count file for each table. Can be directly used for computing the tf-idf features \n",
    "#without running the candidate generation process again which is expensive\n",
    "\n",
    "#GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/train_prop_count/\n",
    "train_prop_count = f'{temp_dir}/train_prop_count/' \n",
    "#GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/dev_prop_count/\n",
    "dev_prop_count = f'{temp_dir}/dev_prop_count/'\n",
    "\n",
    "#GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/train_class_count/\n",
    "train_class_count = f'{temp_dir}/train_class_count/'\n",
    "#GDrive Path: /table-linker-dataset/2019-iswc_challenge_data/t2dv2/canonical-with-context/dev_class_count/\n",
    "dev_class_count = f'{temp_dir}/dev_class_count/'\n",
    "\n",
    "train_graph_embedding = f'{temp_dir}/train_graph_embedding/'\n",
    "dev_graph_embedding = f'{temp_dir}/dev_graph_embedding/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "supreme-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $temp_dir\n",
    "!mkdir -p $train_prop_count\n",
    "!mkdir -p $dev_prop_count\n",
    "!mkdir -p $train_class_count\n",
    "!mkdir -p $dev_class_count\n",
    "!mkdir -p $train_graph_embedding\n",
    "!mkdir -p $dev_graph_embedding\n",
    "!mkdir -p $train_candidate_path\n",
    "!mkdir -p $dev_candidate_path\n",
    "!mkdir -p $train_feature_path\n",
    "!mkdir -p $dev_feature_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opening-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_generation(path, gt_path, output_path, class_count, prop_count, graph_embedding):\n",
    "    for i, file in enumerate(glob.glob(path + '*.csv')):\n",
    "        st = time.time()\n",
    "        filename = file.split('/')[-1]\n",
    "        print(filename)\n",
    "        gt_file = os.path.join(ground_truth_files, filename)\n",
    "        output_file = os.path.join(output_path, filename)\n",
    "        \n",
    "        !tl clean -c label -o label_clean $file / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-fuzzy-augmented-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder $temp_dir / \\\n",
    "        --url $es_url --index $es_index \\\n",
    "        get-exact-matches -c label_clean \\\n",
    "        --auxiliary-fields {aux_field} \\\n",
    "        --auxiliary-folder {temp_dir} / \\\n",
    "        ground-truth-labeler --gt-file $gt_file > $output_file\n",
    "        \n",
    "        for field in aux_field.split(','):\n",
    "            aux_list = []\n",
    "            for f in glob.glob(f'{temp_dir}/*{field}.tsv'):\n",
    "                aux_list.append(pd.read_csv(f, sep='\\t', dtype=object))\n",
    "            aux_df = pd.concat(aux_list).drop_duplicates(subset=['qnode'])\n",
    "            if field == 'class_count':\n",
    "                class_count_file = os.path.join(class_count, filename.strip('.csv') + '_class_count.tsv')\n",
    "                aux_df.to_csv(class_count_file, sep='\\t', index=False)\n",
    "            elif field == 'property_count':\n",
    "                prop_count_file = os.path.join(prop_count, filename.strip('.csv') + '_prop_count.tsv')\n",
    "                aux_df.to_csv(prop_count_file, sep='\\t', index=False)\n",
    "            else:\n",
    "                graph_embedding_file = os.path.join(graph_embedding, filename.strip('.csv') + '_graph_embedding_complex.tsv')\n",
    "                aux_df.to_csv(graph_embedding_file, sep='\\t', index=False)\n",
    "        \n",
    "        print(time.time() - st)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lasting-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(candidate_dir, embedding_dir, class_count_dir, property_count_dir, output_path):\n",
    "    print(candidate_dir)\n",
    "    print(embedding_dir)\n",
    "    print(class_count_dir)\n",
    "    print(property_count_dir)\n",
    "    print(output_path)\n",
    "    for file in glob.glob(candidate_dir + '*.csv'):\n",
    "        filename = file.split('/')[-1]\n",
    "        print(filename)\n",
    "        embedding_file = os.path.join(embedding_dir, filename.strip('.csv') + '_graph_embedding_complex.tsv')\n",
    "        class_count_file = f\"{class_count_dir}{filename.strip('.csv')}_class_count.tsv\"\n",
    "        property_count_file = f\"{property_count_dir}{filename.strip('.csv')}_prop_count.tsv\"\n",
    "        output_file = os.path.join(output_path, filename)\n",
    "        !tl align-page-rank $file \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -o monge_elkan \\\n",
    "            / string-similarity -i --method symmetric_monge_elkan:tokenizer=word -c label_clean kg_aliases -o monge_elkan_aliases \\\n",
    "            / string-similarity -i --method jaro_winkler -o jaro_winkler \\\n",
    "            / string-similarity -i --method levenshtein -o levenshtein \\\n",
    "            / string-similarity -i --method jaccard:tokenizer=word -c kg_descriptions context -o des_cont_jaccard \\\n",
    "            / normalize-scores -c des_cont_jaccard / smallest-qnode-number \\\n",
    "            / mosaic-features -c kg_labels --num-char --num-tokens \\\n",
    "            / create-singleton-feature -o singleton \\\n",
    "            / vote-by-classifier  \\\n",
    "            --prob-threshold 0.995 \\\n",
    "            --model $classifier_model_path \\\n",
    "            / score-using-embedding \\\n",
    "            --column-vector-strategy centroid-of-lof \\\n",
    "            --lof-strategy ems-mv \\\n",
    "            -o lof-graph-embedding-score \\\n",
    "            --embedding-file $embedding_file \\\n",
    "            --embedding-url \"$es_url/$es_index/\" \\\n",
    "            / generate-reciprocal-rank  \\\n",
    "            -c lof-graph-embedding-score \\\n",
    "            -o lof-reciprocal-rank \\\n",
    "            / compute-tf-idf  \\\n",
    "            --feature-file $class_count_file \\\n",
    "            --feature-name class_count \\\n",
    "            --singleton-column singleton \\\n",
    "            -o lof_class_count_tf_idf_score \\\n",
    "            / compute-tf-idf \\\n",
    "            --feature-file $property_count_file \\\n",
    "            --feature-name property_count \\\n",
    "            --singleton-column singleton \\\n",
    "            -o lof_property_count_tf_idf_score \\\n",
    "            > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "['pagerank','retrieval_score','monge_elkan','des_cont_jaccard',\n",
    "            'jaro_winkler','levenshtein','singleton','is_lof','num_char','num_tokens',\n",
    "           'lof_class_count_tf_idf_score', 'lof_property_count_tf_idf_score',\n",
    "           'lof-graph-embedding-score', 'lof-reciprocal-rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "searching-alexandria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv\n",
      "215.0770800113678\n",
      "39173938_0_7916056990138658530.csv\n",
      "204.2746980190277\n",
      "10579449_0_1681126353774891032.csv\n",
      "41.339812994003296\n",
      "33401079_0_9127583903019856402.csv\n",
      "195.1763949394226\n",
      "21362676_0_6854186738074119688.csv\n",
      "225.82092714309692\n",
      "38428277_0_1311643810102462607.csv\n",
      "247.50702118873596\n",
      "91959037_0_7907661684242014480.csv\n",
      "865.6964118480682\n",
      "20135078_0_7570343137119682530.csv\n",
      "215.20253586769104\n",
      "35188621_0_6058553107571275232.csv\n",
      "230.3643729686737\n",
      "54719588_0_8417197176086756912.csv\n",
      "553.5450069904327\n",
      "21245481_0_8730460088443117515.csv\n",
      "480.3478729724884\n",
      "71840765_0_6664391841933033844.csv\n",
      "35.67017602920532\n",
      "8468806_0_4382447409703007384.csv\n",
      "167.83368825912476\n",
      "88523363_0_8180214313099580515.csv\n",
      "1107.9533722400665\n",
      "29414811_13_8724394428539174350.csv\n",
      "38.97614812850952\n",
      "99070098_0_2074872741302696997.csv\n",
      "449.4610207080841\n",
      "43237185_1_3636357855502246981.csv\n",
      "67.81162977218628\n",
      "46671561_0_6122315295162029872.csv\n",
      "484.14470076560974\n",
      "53989675_0_8697482470743954630.csv\n",
      "89.17002582550049\n",
      "25404227_0_2240631045609013057.csv\n",
      "289.8515841960907\n",
      "9834884_0_3871985887467090123.csv\n",
      "1065.960769891739\n",
      "63450419_0_8012592961815711786.csv\n",
      "366.36851263046265\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "28.90010714530945\n",
      "22864497_0_8632623712684511496.csv\n",
      "849.5500421524048\n",
      "53822652_0_5767892317858575530.csv\n",
      "1030.43421626091\n",
      "37856682_0_6818907050314633217.csv\n",
      "834.2968180179596\n",
      "26310680_0_5150772059999313798.csv\n",
      "578.7745349407196\n",
      "29414811_12_251152470253168163.csv\n",
      "58.183043003082275\n",
      "69537082_0_7789694313271016902.csv\n",
      "531.0336751937866\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "22.25050711631775\n",
      "60319454_0_3938426910282115527.csv\n",
      "115.3537061214447\n",
      "16767252_0_2409448375013995751.csv\n",
      "187.1744360923767\n",
      "84548468_0_5955155464119382182.csv\n",
      "231.16681718826294\n",
      "80588006_0_6965325215443683359.csv\n",
      "44.82205510139465\n",
      "39650055_5_7135804139753401681.csv\n",
      "280.3572018146515\n",
      "40534006_0_4617468856744635526.csv\n",
      "88.80142402648926\n",
      "90196673_0_5458330029110291950.csv\n",
      "730.7326078414917\n",
      "24036779_0_5608105867560183058.csv\n",
      "199.4069151878357\n",
      "9567241_0_5666388268510912770.csv\n",
      "51.12745189666748\n",
      "41480166_0_6681239260286218499.csv\n",
      "457.9718391895294\n",
      "77694908_0_6083291340991074532.csv\n",
      "214.27361607551575\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "40.07312798500061\n",
      "39107734_2_2329160387535788734.csv\n",
      "126.85119605064392\n",
      "50245608_0_871275842592178099.csv\n",
      "592.9691939353943\n"
     ]
    }
   ],
   "source": [
    "candidate_generation(train_path, ground_truth_files, train_candidate_path, train_class_count, train_prop_count, train_graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reduced-ordering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "221.68012309074402\n",
      "45073662_0_3179937335063201739.csv\n",
      "71.87427115440369\n",
      "29414811_2_4773219892816395776.csv\n",
      "50.534576177597046\n",
      "84575189_0_6365692015941409487.csv\n",
      "272.8969089984894\n",
      "14380604_4_3329235705746762392.csv\n",
      "46.958092212677\n",
      "52299421_0_4473286348258170200.csv\n",
      "195.72647094726562\n",
      "50270082_0_444360818941411589.csv\n",
      "379.41621804237366\n",
      "28086084_0_3127660530989916727.csv\n",
      "425.98705887794495\n",
      "14067031_0_559833072073397908.csv\n",
      "109.25832009315491\n"
     ]
    }
   ],
   "source": [
    "candidate_generation(dev_path, ground_truth_files, dev_candidate_path, dev_class_count, dev_prop_count, dev_graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "constitutional-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/data/t2dv2/t2dv2-train-canonical/output-candidates/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//train_graph_embedding/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//train_class_count/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//train_prop_count/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/t2dv2-train-canonical/output-features-2/\n",
      "58891288_0_1117541047012405958.csv\n",
      "Qnodes to lookup: 10717\n",
      "Qnodes from file: 10399\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 86 lof-voted candidates\n",
      "39173938_0_7916056990138658530.csv\n",
      "Qnodes to lookup: 9986\n",
      "Qnodes from file: 9718\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 73 lof-voted candidates\n",
      "10579449_0_1681126353774891032.csv\n",
      "Qnodes to lookup: 1706\n",
      "Qnodes from file: 1652\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 158, in process_vectors\n",
      "    if not self._centroid_of_lof():\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 330, in _centroid_of_lof\n",
      "    lof_pred = clf.fit_predict(vectors)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 246, in _fit_predict\n",
      "    return self.fit(X)._predict()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 265, in fit\n",
      "    self._fit(X)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 514, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors > 0. Got 0\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "33401079_0_9127583903019856402.csv\n",
      "Qnodes to lookup: 8276\n",
      "Qnodes from file: 8183\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 50 lof-voted candidates\n",
      "21362676_0_6854186738074119688.csv\n",
      "Qnodes to lookup: 10157\n",
      "Qnodes from file: 9877\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 104 lof-voted candidates\n",
      "38428277_0_1311643810102462607.csv\n",
      "Qnodes to lookup: 12595\n",
      "Qnodes from file: 12188\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 115 lof-voted candidates\n",
      "91959037_0_7907661684242014480.csv\n",
      "Qnodes to lookup: 22297\n",
      "Qnodes from file: 21621\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 20 lof-voted candidates\n",
      "20135078_0_7570343137119682530.csv\n",
      "Qnodes to lookup: 11208\n",
      "Qnodes from file: 10916\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 101 lof-voted candidates\n",
      "35188621_0_6058553107571275232.csv\n",
      "Qnodes to lookup: 11833\n",
      "Qnodes from file: 11460\n",
      "Qnodes from server: 0\n",
      "_centroid_of_lof: Missing 1 of 164\n",
      "Outlier removal generates 98 lof-voted candidates\n",
      "54719588_0_8417197176086756912.csv\n",
      "Qnodes to lookup: 24211\n",
      "Qnodes from file: 23278\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 45 lof-voted candidates\n",
      "21245481_0_8730460088443117515.csv\n",
      "Qnodes to lookup: 11848\n",
      "Qnodes from file: 11696\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 17 lof-voted candidates\n",
      "71840765_0_6664391841933033844.csv\n",
      "Qnodes to lookup: 1100\n",
      "Qnodes from file: 1094\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "8468806_0_4382447409703007384.csv\n",
      "Qnodes to lookup: 6738\n",
      "Qnodes from file: 6671\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 9 lof-voted candidates\n",
      "88523363_0_8180214313099580515.csv\n",
      "Qnodes to lookup: 42234\n",
      "Qnodes from file: 41968\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 35 lof-voted candidates\n",
      "29414811_13_8724394428539174350.csv\n",
      "Qnodes to lookup: 1140\n",
      "Qnodes from file: 1102\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 2 lof-voted candidates\n",
      "99070098_0_2074872741302696997.csv\n",
      "Qnodes to lookup: 12078\n",
      "Qnodes from file: 11859\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 15 lof-voted candidates\n",
      "43237185_1_3636357855502246981.csv\n",
      "Qnodes to lookup: 2611\n",
      "Qnodes from file: 2573\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "46671561_0_6122315295162029872.csv\n",
      "Qnodes to lookup: 15698\n",
      "Qnodes from file: 15130\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 66 lof-voted candidates\n",
      "53989675_0_8697482470743954630.csv\n",
      "Qnodes to lookup: 2161\n",
      "Qnodes from file: 2144\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 158, in process_vectors\n",
      "    if not self._centroid_of_lof():\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 330, in _centroid_of_lof\n",
      "    lof_pred = clf.fit_predict(vectors)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 246, in _fit_predict\n",
      "    return self.fit(X)._predict()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 265, in fit\n",
      "    self._fit(X)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 514, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors > 0. Got 0\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "25404227_0_2240631045609013057.csv\n",
      "Qnodes to lookup: 10026\n",
      "Qnodes from file: 9732\n",
      "Qnodes from server: 0\n",
      "_centroid_of_lof: Missing 1 of 160\n",
      "Outlier removal generates 96 lof-voted candidates\n",
      "9834884_0_3871985887467090123.csv\n",
      "Qnodes to lookup: 22923\n",
      "Qnodes from file: 22574\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 56 lof-voted candidates\n",
      "63450419_0_8012592961815711786.csv\n",
      "Qnodes to lookup: 13769\n",
      "Qnodes from file: 13691\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "Qnodes to lookup: 1237\n",
      "Qnodes from file: 1179\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 4 lof-voted candidates\n",
      "22864497_0_8632623712684511496.csv\n",
      "Qnodes to lookup: 29456\n",
      "Qnodes from file: 28330\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 48 lof-voted candidates\n",
      "53822652_0_5767892317858575530.csv\n",
      "Qnodes to lookup: 43140\n",
      "Qnodes from file: 41723\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 76 lof-voted candidates\n",
      "37856682_0_6818907050314633217.csv\n",
      "Qnodes to lookup: 43381\n",
      "Qnodes from file: 42173\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 272 lof-voted candidates\n",
      "26310680_0_5150772059999313798.csv\n",
      "Qnodes to lookup: 35193\n",
      "Qnodes from file: 34092\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 200 lof-voted candidates\n",
      "29414811_12_251152470253168163.csv\n",
      "Qnodes to lookup: 1857\n",
      "Qnodes from file: 1790\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "69537082_0_7789694313271016902.csv\n",
      "Qnodes to lookup: 36385\n",
      "Qnodes from file: 35280\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 226 lof-voted candidates\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "Qnodes to lookup: 980\n",
      "Qnodes from file: 976\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 2 lof-voted candidates\n",
      "60319454_0_3938426910282115527.csv\n",
      "Qnodes to lookup: 5005\n",
      "Qnodes from file: 4886\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 11 lof-voted candidates\n",
      "16767252_0_2409448375013995751.csv\n",
      "Qnodes to lookup: 8870\n",
      "Qnodes from file: 8626\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 88 lof-voted candidates\n",
      "84548468_0_5955155464119382182.csv\n",
      "Qnodes to lookup: 10574\n",
      "Qnodes from file: 10229\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 78 lof-voted candidates\n",
      "80588006_0_6965325215443683359.csv\n",
      "Qnodes to lookup: 1855\n",
      "Qnodes from file: 1809\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "39650055_5_7135804139753401681.csv\n",
      "Qnodes to lookup: 13681\n",
      "Qnodes from file: 13068\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 30 lof-voted candidates\n",
      "40534006_0_4617468856744635526.csv\n",
      "Qnodes to lookup: 4092\n",
      "Qnodes from file: 4034\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 10 lof-voted candidates\n",
      "90196673_0_5458330029110291950.csv\n",
      "Qnodes to lookup: 22360\n",
      "Qnodes from file: 21926\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 21 lof-voted candidates\n",
      "24036779_0_5608105867560183058.csv\n",
      "Qnodes to lookup: 13908\n",
      "Qnodes from file: 13592\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 114 lof-voted candidates\n",
      "9567241_0_5666388268510912770.csv\n",
      "Qnodes to lookup: 1922\n",
      "Qnodes from file: 1861\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "41480166_0_6681239260286218499.csv\n",
      "Qnodes to lookup: 19531\n",
      "Qnodes from file: 19257\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "77694908_0_6083291340991074532.csv\n",
      "Qnodes to lookup: 10760\n",
      "Qnodes from file: 10387\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 96 lof-voted candidates\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "Qnodes to lookup: 1234\n",
      "Qnodes from file: 1218\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 159, in process_vectors\n",
      "    raise TLException(f'Column_vector_stragtegy {vector_strategy} failed')\n",
      "tl.exceptions.TLException: Column_vector_stragtegy centroid-of-lof failed\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "39107734_2_2329160387535788734.csv\n",
      "Qnodes to lookup: 3480\n",
      "Qnodes from file: 3403\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 42 lof-voted candidates\n",
      "50245608_0_871275842592178099.csv\n",
      "Qnodes to lookup: 26520\n",
      "Qnodes from file: 25679\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 151 lof-voted candidates\n"
     ]
    }
   ],
   "source": [
    "feature_generation(train_candidate_path, train_graph_embedding, train_class_count, train_prop_count, train_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decreased-possession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amandeep/Github/table-linker/data/t2dv2/t2dv2-dev-canonical/output-candidates/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//dev_graph_embedding/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//dev_class_count/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/temp//dev_prop_count/\n",
      "/Users/amandeep/Github/table-linker/data/t2dv2/t2dv2-dev-canonical/output-features-2/\n",
      "39759273_0_1427898308030295194.csv\n",
      "Qnodes to lookup: 10448\n",
      "Qnodes from file: 10120\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 101 lof-voted candidates\n",
      "45073662_0_3179937335063201739.csv\n",
      "Qnodes to lookup: 3040\n",
      "Qnodes from file: 3004\n",
      "Qnodes from server: 0\n",
      "Command: score-using-embedding\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/score-using-embedding.py\", line 74, in run\n",
      "    vector_transformer.process_vectors()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 158, in process_vectors\n",
      "    if not self._centroid_of_lof():\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/external_embedding.py\", line 330, in _centroid_of_lof\n",
      "    lof_pred = clf.fit_predict(vectors)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 246, in _fit_predict\n",
      "    return self.fit(X)._predict()\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_lof.py\", line 265, in fit\n",
      "    self._fit(X)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 514, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors > 0. Got 0\n",
      "\n",
      "Command: generate-reciprocal-rank\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/generate-reciprocal-rank.py\", line 35, in run\n",
      "    df = pd.read_csv(kwargs['input_file'], dtype=object)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "Command: compute-tf-idf\n",
      "Error Message:  Traceback (most recent call last):\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/cli/compute-tf-idf.py\", line 44, in run\n",
      "    tfidf_unit = tfidf.TFIDF(kwargs['output_column_name'],\n",
      "  File \"/Users/amandeep/Github/table-linker/tl/features/tfidf.py\", line 21, in __init__\n",
      "    self.input_df = pd.read_csv(input_file)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/amandeep/Github/table-linker/tl_env/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1898, in __init__\n",
      "    self._reader = parsers.TextReader(self.handles.handle, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 521, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "29414811_2_4773219892816395776.csv\n",
      "Qnodes to lookup: 2106\n",
      "Qnodes from file: 2025\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 20 lof-voted candidates\n",
      "84575189_0_6365692015941409487.csv\n",
      "Qnodes to lookup: 8486\n",
      "Qnodes from file: 7897\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 8 lof-voted candidates\n",
      "14380604_4_3329235705746762392.csv\n",
      "Qnodes to lookup: 2291\n",
      "Qnodes from file: 2226\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 7 lof-voted candidates\n",
      "52299421_0_4473286348258170200.csv\n",
      "Qnodes to lookup: 15531\n",
      "Qnodes from file: 15218\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 83 lof-voted candidates\n",
      "50270082_0_444360818941411589.csv\n",
      "Qnodes to lookup: 16794\n",
      "Qnodes from file: 16683\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 5 lof-voted candidates\n",
      "28086084_0_3127660530989916727.csv\n",
      "Qnodes to lookup: 19531\n",
      "Qnodes from file: 19257\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 19 lof-voted candidates\n",
      "14067031_0_559833072073397908.csv\n",
      "Qnodes to lookup: 7690\n",
      "Qnodes from file: 7433\n",
      "Qnodes from server: 0\n",
      "Outlier removal generates 56 lof-voted candidates\n"
     ]
    }
   ],
   "source": [
    "feature_generation(dev_candidate_path, dev_graph_embedding, dev_class_count, dev_prop_count, dev_feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-competition",
   "metadata": {},
   "source": [
    "### Generate Balanced Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "frank-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datapath = '../random_forest_ranking/training_data_dwd.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "occasional-tiffany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv\n",
      "39173938_0_7916056990138658530.csv\n",
      "10579449_0_1681126353774891032.csv\n",
      "33401079_0_9127583903019856402.csv\n",
      "21362676_0_6854186738074119688.csv\n",
      "38428277_0_1311643810102462607.csv\n",
      "91959037_0_7907661684242014480.csv\n",
      "20135078_0_7570343137119682530.csv\n",
      "35188621_0_6058553107571275232.csv\n",
      "54719588_0_8417197176086756912.csv\n",
      "21245481_0_8730460088443117515.csv\n",
      "71840765_0_6664391841933033844.csv\n",
      "8468806_0_4382447409703007384.csv\n",
      "88523363_0_8180214313099580515.csv\n",
      "29414811_13_8724394428539174350.csv\n",
      "99070098_0_2074872741302696997.csv\n",
      "43237185_1_3636357855502246981.csv\n",
      "46671561_0_6122315295162029872.csv\n",
      "53989675_0_8697482470743954630.csv\n",
      "25404227_0_2240631045609013057.csv\n",
      "9834884_0_3871985887467090123.csv\n",
      "63450419_0_8012592961815711786.csv\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "22864497_0_8632623712684511496.csv\n",
      "53822652_0_5767892317858575530.csv\n",
      "37856682_0_6818907050314633217.csv\n",
      "26310680_0_5150772059999313798.csv\n",
      "29414811_12_251152470253168163.csv\n",
      "69537082_0_7789694313271016902.csv\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "60319454_0_3938426910282115527.csv\n",
      "16767252_0_2409448375013995751.csv\n",
      "84548468_0_5955155464119382182.csv\n",
      "80588006_0_6965325215443683359.csv\n",
      "39650055_5_7135804139753401681.csv\n",
      "40534006_0_4617468856744635526.csv\n",
      "90196673_0_5458330029110291950.csv\n",
      "24036779_0_5608105867560183058.csv\n",
      "9567241_0_5666388268510912770.csv\n",
      "41480166_0_6681239260286218499.csv\n",
      "77694908_0_6083291340991074532.csv\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "39107734_2_2329160387535788734.csv\n",
      "50245608_0_871275842592178099.csv\n"
     ]
    }
   ],
   "source": [
    "final_list = []\n",
    "for i,file in enumerate(glob.glob(train_candidate_path + '*.csv')):\n",
    "    file_name = file.split('/')[-1]\n",
    "    print(file_name)\n",
    "    \n",
    "    try:\n",
    "        d_sample = pd.read_csv(file)\n",
    "        grouped_obj = d_sample.groupby(['row', 'column'])\n",
    "        for cell in grouped_obj:\n",
    "            num_rows = random.randint(2,5)\n",
    "            sorted_df = cell[1].sort_values('graph-embedding-score',ascending=False)\n",
    "            if 0 in sorted_df['evaluation_label'].tolist():\n",
    "                continue\n",
    "            if sorted_df.empty:\n",
    "                continue\n",
    "            if num_rows < len(sorted_df):\n",
    "                top_sample_df = sorted_df[sorted_df['evaluation_label'] == -1][:10].sample(n=num_rows)\n",
    "                bottom_sample_df = sorted_df[sorted_df['evaluation_label'] == -1][-10:].sample(n=num_rows)\n",
    "                final_list.extend(top_sample_df.to_dict(orient='records'))\n",
    "                final_list.extend(bottom_sample_df.to_dict(orient='records'))\n",
    "            else:\n",
    "                sample_df = sorted_df[sorted_df['evaluation_label'] == -1]\n",
    "                final_list.extend(sample_df.to_dict(orient='records'))\n",
    "            a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "            if a.empty:\n",
    "                continue\n",
    "            final_list.extend(a.to_dict(orient='records'))\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "train_df = pd.DataFrame(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aggregate-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(training_datapath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-difficulty",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "universal-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48365 entries, 0 to 48364\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   column                 48365 non-null  int64  \n",
      " 1   row                    48365 non-null  int64  \n",
      " 2   label                  48363 non-null  object \n",
      " 3   context                48353 non-null  object \n",
      " 4   label_clean            48363 non-null  object \n",
      " 5   kg_id                  47784 non-null  object \n",
      " 6   kg_labels              46828 non-null  object \n",
      " 7   kg_aliases             14414 non-null  object \n",
      " 8   method                 48365 non-null  object \n",
      " 9   kg_descriptions        39166 non-null  object \n",
      " 10  pagerank               48365 non-null  float64\n",
      " 11  retrieval_score        48365 non-null  float64\n",
      " 12  GT_kg_id               48365 non-null  object \n",
      " 13  GT_kg_label            48365 non-null  object \n",
      " 14  evaluation_label       48365 non-null  int64  \n",
      " 15  monge_elkan            48365 non-null  float64\n",
      " 16  des_cont_jaccard       48365 non-null  float64\n",
      " 17  jaro_winkler           48365 non-null  float64\n",
      " 18  graph-embedding-score  48365 non-null  float64\n",
      " 19  singleton              48365 non-null  int64  \n",
      " 20  reciprocal_rank        48365 non-null  float64\n",
      " 21  num_char               48365 non-null  int64  \n",
      " 22  num_tokens             48365 non-null  int64  \n",
      "dtypes: float64(7), int64(6), object(10)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(train_datapath)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "threaded-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48365 entries, 0 to 48364\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   pagerank               48365 non-null  float64\n",
      " 1   retrieval_score        48365 non-null  float64\n",
      " 2   monge_elkan            48365 non-null  float64\n",
      " 3   des_cont_jaccard       48365 non-null  float64\n",
      " 4   jaro_winkler           48365 non-null  float64\n",
      " 5   graph-embedding-score  48365 non-null  float64\n",
      " 6   singleton              48365 non-null  int64  \n",
      " 7   num_char               48365 non-null  int64  \n",
      " 8   num_tokens             48365 non-null  int64  \n",
      " 9   reciprocal_rank        48365 non-null  float64\n",
      "dtypes: float64(7), int64(3)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Features we need to include in training\n",
    "features = ['pagerank','retrieval_score','monge_elkan',\n",
    "            'des_cont_jaccard','jaro_winkler','graph-embedding-score',\n",
    "           'singleton','num_char','num_tokens','reciprocal_rank']\n",
    "evaluation_label = ['evaluation_label']\n",
    "\n",
    "df[features].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adopted-proof",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48365 entries, 0 to 48364\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   pagerank               48365 non-null  float64\n",
      " 1   retrieval_score        48365 non-null  float64\n",
      " 2   monge_elkan            48365 non-null  float64\n",
      " 3   des_cont_jaccard       48365 non-null  float64\n",
      " 4   jaro_winkler           48365 non-null  float64\n",
      " 5   graph-embedding-score  48365 non-null  float64\n",
      " 6   singleton              48365 non-null  int64  \n",
      " 7   num_char               48365 non-null  int64  \n",
      " 8   num_tokens             48365 non-null  int64  \n",
      " 9   reciprocal_rank        48365 non-null  float64\n",
      "dtypes: float64(7), int64(3)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "df['graph-embedding-score'] = df['graph-embedding-score'].fillna(0.0)\n",
    "df['reciprocal_rank'] = df['reciprocal_rank'].fillna(0.0)\n",
    "df[features].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-settlement",
   "metadata": {},
   "source": [
    "### Train a Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "psychological-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[features]\n",
    "y_label = df[evaluation_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "proved-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rijulvohra/opt/anaconda3/envs/table_linker_dev/lib/python3.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, max_features=\"log2\",min_samples_leaf=3)\n",
    "model.fit(train_data,y_label)\n",
    "y_pred = model.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "posted-daughter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036428866339495325"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_label, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "japanese-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '../random_forest_ranking/rf_tuned_dwd_ranking.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open(model_save_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dangerous-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = pickle.load(open(model_save_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-employment",
   "metadata": {},
   "source": [
    "### Predicting Scores for Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "judicial-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_output = '/Users/rijulvohra/Documents/work/Novartis-ISI/novartis-isi-git/entity_linking/t2dv2-raw/t2dv2/canonical-with-context/t2dv2-train-rf-pred-dwd-2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fresh-saver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58891288_0_1117541047012405958.csv\n",
      "39173938_0_7916056990138658530.csv\n",
      "10579449_0_1681126353774891032.csv\n",
      "33401079_0_9127583903019856402.csv\n",
      "21362676_0_6854186738074119688.csv\n",
      "38428277_0_1311643810102462607.csv\n",
      "91959037_0_7907661684242014480.csv\n",
      "20135078_0_7570343137119682530.csv\n",
      "35188621_0_6058553107571275232.csv\n",
      "54719588_0_8417197176086756912.csv\n",
      "21245481_0_8730460088443117515.csv\n",
      "71840765_0_6664391841933033844.csv\n",
      "8468806_0_4382447409703007384.csv\n",
      "88523363_0_8180214313099580515.csv\n",
      "29414811_13_8724394428539174350.csv\n",
      "99070098_0_2074872741302696997.csv\n",
      "43237185_1_3636357855502246981.csv\n",
      "46671561_0_6122315295162029872.csv\n",
      "53989675_0_8697482470743954630.csv\n",
      "25404227_0_2240631045609013057.csv\n",
      "9834884_0_3871985887467090123.csv\n",
      "63450419_0_8012592961815711786.csv\n",
      "1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "22864497_0_8632623712684511496.csv\n",
      "53822652_0_5767892317858575530.csv\n",
      "37856682_0_6818907050314633217.csv\n",
      "26310680_0_5150772059999313798.csv\n",
      "29414811_12_251152470253168163.csv\n",
      "69537082_0_7789694313271016902.csv\n",
      "1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "60319454_0_3938426910282115527.csv\n",
      "16767252_0_2409448375013995751.csv\n",
      "84548468_0_5955155464119382182.csv\n",
      "80588006_0_6965325215443683359.csv\n",
      "39650055_5_7135804139753401681.csv\n",
      "40534006_0_4617468856744635526.csv\n",
      "90196673_0_5458330029110291950.csv\n",
      "24036779_0_5608105867560183058.csv\n",
      "9567241_0_5666388268510912770.csv\n",
      "41480166_0_6681239260286218499.csv\n",
      "77694908_0_6083291340991074532.csv\n",
      "1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "39107734_2_2329160387535788734.csv\n",
      "50245608_0_871275842592178099.csv\n"
     ]
    }
   ],
   "source": [
    "train_mse = []\n",
    "for file in glob.glob(train_candidate_path + '*.csv'):\n",
    "    try:\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(file_name)\n",
    "        df_file = pd.read_csv(file)\n",
    "        data = df_file[features]\n",
    "        y_file_label = df_file[evaluation_label]\n",
    "        y_file_pred = saved_model.predict(data)\n",
    "        df_file['rf_model_pred'] = y_file_pred\n",
    "        file_mse = mean_squared_error(y_file_label,y_file_pred)\n",
    "        train_mse.append(file_mse)\n",
    "        df_file.to_csv(os.path.join(train_pred_output,file_name),index=False)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "affiliated-edmonton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE is:  0.4519201624197591\n"
     ]
    }
   ],
   "source": [
    "print(\"Train MSE is: \", sum(train_mse)/len(train_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-palmer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "formal-weekend",
   "metadata": {},
   "source": [
    "### Predicting Scores for dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "innocent-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_output = '/Users/rijulvohra/Documents/work/Novartis-ISI/novartis-isi-git/entity_linking/t2dv2-raw/t2dv2/canonical-with-context/t2dv2-dev-rf-pred-dwd-2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ideal-metabolism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39759273_0_1427898308030295194.csv\n",
      "45073662_0_3179937335063201739.csv\n",
      "29414811_2_4773219892816395776.csv\n",
      "84575189_0_6365692015941409487.csv\n",
      "14380604_4_3329235705746762392.csv\n",
      "50270082_0_444360818941411589.csv\n",
      "28086084_0_3127660530989916727.csv\n",
      "14067031_0_559833072073397908.csv\n"
     ]
    }
   ],
   "source": [
    "dev_mse = []\n",
    "for file in glob.glob(dev_candidate_path + '*.csv'):\n",
    "\n",
    "    file_name = file.split('/')[-1]\n",
    "    print(file_name)\n",
    "    df_file = pd.read_csv(file)\n",
    "    data = df_file[features]\n",
    "    y_file_label = df_file[evaluation_label]\n",
    "    y_file_pred = saved_model.predict(data)\n",
    "    df_file['rf_model_pred'] = y_file_pred\n",
    "    file_mse = mean_squared_error(y_file_label,y_file_pred)\n",
    "    dev_mse.append(file_mse)\n",
    "    df_file.to_csv(os.path.join(dev_pred_output,file_name),index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "noticed-positive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev MSE is:  0.40129949369858403\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev MSE is: \", sum(dev_mse)/len(dev_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-oriental",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lined-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score_path = train_pred_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "international-january",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "eval_file_names = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(final_score_path):\n",
    "    for fn in filenames:\n",
    "        if \"csv\" not in fn:\n",
    "            continue\n",
    "        abs_fn = dirpath + fn\n",
    "        assert os.path.isfile(abs_fn)\n",
    "        if os.path.getsize(abs_fn) == 0:\n",
    "            continue\n",
    "        eval_file_names.append(abs_fn)\n",
    "len(eval_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "soviet-discretion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>row</th>\n",
       "      <th>label</th>\n",
       "      <th>context</th>\n",
       "      <th>label_clean</th>\n",
       "      <th>kg_id</th>\n",
       "      <th>kg_labels</th>\n",
       "      <th>kg_aliases</th>\n",
       "      <th>method</th>\n",
       "      <th>kg_descriptions</th>\n",
       "      <th>...</th>\n",
       "      <th>monge_elkan</th>\n",
       "      <th>des_cont_jaccard</th>\n",
       "      <th>jaro_winkler</th>\n",
       "      <th>graph-embedding-score</th>\n",
       "      <th>singleton</th>\n",
       "      <th>reciprocal_rank</th>\n",
       "      <th>num_char</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>rf_model_pred</th>\n",
       "      <th>table_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1|1972|Francis Ford Coppola|1</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Q28452137</td>\n",
       "      <td>Godfather</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fuzzy-augmented</td>\n",
       "      <td>album by Wiley</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.350448</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.976667</td>\n",
       "      <td>58891288_0_1117541047012405958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1|1972|Francis Ford Coppola|1</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Q1066512</td>\n",
       "      <td>Charles Wright</td>\n",
       "      <td>The Godfather|Papa Shango</td>\n",
       "      <td>fuzzy-augmented</td>\n",
       "      <td>American professional wrestler</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584249</td>\n",
       "      <td>0.290255</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>58891288_0_1117541047012405958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1|1972|Francis Ford Coppola|1</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Q47703</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Godfather|The Godfather Part I</td>\n",
       "      <td>fuzzy-augmented</td>\n",
       "      <td>1972 American film directed by Francis Ford Co...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783063</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>58891288_0_1117541047012405958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1|1972|Francis Ford Coppola|1</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Q6144534</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fuzzy-augmented</td>\n",
       "      <td>2002 album</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468657</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.930031</td>\n",
       "      <td>58891288_0_1117541047012405958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1|1972|Francis Ford Coppola|1</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Q5205330</td>\n",
       "      <td>DJ Godfather</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fuzzy-augmented</td>\n",
       "      <td>American DJ and record producer</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800855</td>\n",
       "      <td>0.257788</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>58891288_0_1117541047012405958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33011</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>1993|DVD|Harold Ramis|Danny Rubin|101|Widescreen</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>Q1210286</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exact-match</td>\n",
       "      <td>Wikimedia disambiguation page</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.676114</td>\n",
       "      <td>50245608_0_871275842592178099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33012</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>1993|DVD|Harold Ramis|Danny Rubin|101|Widescreen</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>Q18614990</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>xkcd 1076</td>\n",
       "      <td>exact-match</td>\n",
       "      <td>1076th strip of the webcomic xkcd</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.611415</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.300482</td>\n",
       "      <td>50245608_0_871275842592178099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33013</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>1993|DVD|Harold Ramis|Danny Rubin|101|Widescreen</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>Q19961225</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exact-match</td>\n",
       "      <td>Musical comedy based on the film of the same name</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.644078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.976559</td>\n",
       "      <td>50245608_0_871275842592178099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33014</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>1993|DVD|Harold Ramis|Danny Rubin|101|Widescreen</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>Q488655</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>\\\\ Groundhog Day \\\\</td>\n",
       "      <td>exact-match</td>\n",
       "      <td>1993 comedy film directed by Harold Ramis</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815621</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50245608_0_871275842592178099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33015</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>1993|DVD|Harold Ramis|Danny Rubin|101|Widescreen</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>Q744374</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>Daks Day|Groundhog\\s Day</td>\n",
       "      <td>exact-match</td>\n",
       "      <td>traditional method of weather prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.412008</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.518971</td>\n",
       "      <td>50245608_0_871275842592178099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>774370 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       column  row          label  \\\n",
       "0           1    0  The Godfather   \n",
       "1           1    0  The Godfather   \n",
       "2           1    0  The Godfather   \n",
       "3           1    0  The Godfather   \n",
       "4           1    0  The Godfather   \n",
       "...       ...  ...            ...   \n",
       "33011       0   99  Groundhog Day   \n",
       "33012       0   99  Groundhog Day   \n",
       "33013       0   99  Groundhog Day   \n",
       "33014       0   99  Groundhog Day   \n",
       "33015       0   99  Groundhog Day   \n",
       "\n",
       "                                                context    label_clean  \\\n",
       "0                         1|1972|Francis Ford Coppola|1  The Godfather   \n",
       "1                         1|1972|Francis Ford Coppola|1  The Godfather   \n",
       "2                         1|1972|Francis Ford Coppola|1  The Godfather   \n",
       "3                         1|1972|Francis Ford Coppola|1  The Godfather   \n",
       "4                         1|1972|Francis Ford Coppola|1  The Godfather   \n",
       "...                                                 ...            ...   \n",
       "33011  1993|DVD|Harold Ramis|Danny Rubin|101|Widescreen  Groundhog Day   \n",
       "33012  1993|DVD|Harold Ramis|Danny Rubin|101|Widescreen  Groundhog Day   \n",
       "33013  1993|DVD|Harold Ramis|Danny Rubin|101|Widescreen  Groundhog Day   \n",
       "33014  1993|DVD|Harold Ramis|Danny Rubin|101|Widescreen  Groundhog Day   \n",
       "33015  1993|DVD|Harold Ramis|Danny Rubin|101|Widescreen  Groundhog Day   \n",
       "\n",
       "           kg_id       kg_labels                      kg_aliases  \\\n",
       "0      Q28452137       Godfather                             NaN   \n",
       "1       Q1066512  Charles Wright       The Godfather|Papa Shango   \n",
       "2         Q47703   The Godfather  Godfather|The Godfather Part I   \n",
       "3       Q6144534   The Godfather                             NaN   \n",
       "4       Q5205330    DJ Godfather                             NaN   \n",
       "...          ...             ...                             ...   \n",
       "33011   Q1210286   Groundhog Day                             NaN   \n",
       "33012  Q18614990   Groundhog Day                       xkcd 1076   \n",
       "33013  Q19961225   Groundhog Day                             NaN   \n",
       "33014    Q488655   Groundhog Day             \\\\ Groundhog Day \\\\   \n",
       "33015    Q744374   Groundhog Day        Daks Day|Groundhog\\s Day   \n",
       "\n",
       "                method                                    kg_descriptions  \\\n",
       "0      fuzzy-augmented                                     album by Wiley   \n",
       "1      fuzzy-augmented                     American professional wrestler   \n",
       "2      fuzzy-augmented  1972 American film directed by Francis Ford Co...   \n",
       "3      fuzzy-augmented                                         2002 album   \n",
       "4      fuzzy-augmented                    American DJ and record producer   \n",
       "...                ...                                                ...   \n",
       "33011      exact-match                      Wikimedia disambiguation page   \n",
       "33012      exact-match                  1076th strip of the webcomic xkcd   \n",
       "33013      exact-match  Musical comedy based on the film of the same name   \n",
       "33014      exact-match          1993 comedy film directed by Harold Ramis   \n",
       "33015      exact-match           traditional method of weather prediction   \n",
       "\n",
       "       ...  monge_elkan  des_cont_jaccard jaro_winkler graph-embedding-score  \\\n",
       "0      ...     0.750000          0.000000     0.897436              0.350448   \n",
       "1      ...     0.499339          0.000000     0.584249              0.290255   \n",
       "2      ...     1.000000          0.375000     1.000000              0.783063   \n",
       "3      ...     1.000000          0.000000     1.000000              0.468657   \n",
       "4      ...     0.500000          0.000000     0.800855              0.257788   \n",
       "...    ...          ...               ...          ...                   ...   \n",
       "33011  ...     1.000000          0.000000     1.000000              0.479545   \n",
       "33012  ...     1.000000          0.000000     1.000000              0.611415   \n",
       "33013  ...     1.000000          0.000000     1.000000              0.644078   \n",
       "33014  ...     1.000000          0.285714     1.000000              0.815621   \n",
       "33015  ...     1.000000          0.000000     1.000000              0.412008   \n",
       "\n",
       "       singleton  reciprocal_rank  num_char  num_tokens  rf_model_pred  \\\n",
       "0              0         0.009709         9           1      -0.976667   \n",
       "1              0         0.008929        14           2      -1.000000   \n",
       "2              0         1.000000        13           2       0.996296   \n",
       "3              0         0.011905        13           2       0.930031   \n",
       "4              0         0.008547        12           2      -1.000000   \n",
       "...          ...              ...       ...         ...            ...   \n",
       "33011          0         0.018182        13           2       0.676114   \n",
       "33012          0         0.071429        13           2       0.300482   \n",
       "33013          0         0.111111        13           2      -0.976559   \n",
       "33014          0         0.500000        13           2       1.000000   \n",
       "33015          0         0.013699        13           2       0.518971   \n",
       "\n",
       "                             table_id  \n",
       "0      58891288_0_1117541047012405958  \n",
       "1      58891288_0_1117541047012405958  \n",
       "2      58891288_0_1117541047012405958  \n",
       "3      58891288_0_1117541047012405958  \n",
       "4      58891288_0_1117541047012405958  \n",
       "...                               ...  \n",
       "33011   50245608_0_871275842592178099  \n",
       "33012   50245608_0_871275842592178099  \n",
       "33013   50245608_0_871275842592178099  \n",
       "33014   50245608_0_871275842592178099  \n",
       "33015   50245608_0_871275842592178099  \n",
       "\n",
       "[774370 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge all eval files in one df\n",
    "def merge_df(file_names: list):\n",
    "    df_list = []\n",
    "    for fn in file_names:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = pd.read_csv(fn)\n",
    "        df['table_id'] = fid\n",
    "        # df = df.fillna('')\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "all_data = merge_df(eval_file_names)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "earlier-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse eval file\n",
    "from pandas.core.common import SettingWithCopyError\n",
    "pd.options.mode.chained_assignment = 'raise'\n",
    "\n",
    "def parse_eval_files_stats(eval_data,method):\n",
    "    res = {}\n",
    "    candidate_eval_data = eval_data.groupby(['table_id', 'row', 'column'])['table_id'].count().reset_index(name=\"count\")\n",
    "    res['num_tasks'] = len(eval_data.groupby(['table_id', 'row', 'column']))\n",
    "    res['num_tasks_with_gt'] = len(eval_data[pd.notna(eval_data['GT_kg_id'])].groupby(['table_id', 'row', 'column']))\n",
    "    res['num_tasks_with_gt_in_candidate'] = len(eval_data[eval_data['evaluation_label'] == 1].groupby(['table_id', 'row', 'column']))\n",
    "    res['num_tasks_with_singleton_candidate'] = len(candidate_eval_data[candidate_eval_data['count'] == 1].groupby(['table_id', 'row', 'column']))\n",
    "    singleton_eval_data = candidate_eval_data[candidate_eval_data['count'] == 1]\n",
    "    num_tasks_with_singleton_candidate_with_gt = 0\n",
    "    for i, row in singleton_eval_data.iterrows():\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) == 1\n",
    "        if c_e_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_singleton_candidate_with_gt += 1\n",
    "    res['num_tasks_with_singleton_candidate_with_gt'] = num_tasks_with_singleton_candidate_with_gt\n",
    "    num_tasks_with_graph_top_one_accurate = []\n",
    "    num_tasks_with_graph_top_five_accurate = []\n",
    "    num_tasks_with_graph_top_ten_accurate = []\n",
    "    num_tasks_with_final_score_top_one_accurate = []\n",
    "    num_tasks_with_final_score_top_five_accurate = []\n",
    "    num_tasks_with_final_score_top_ten_accurate = []\n",
    "    num_tasks_with_model_score_top_one_accurate = []\n",
    "    num_tasks_with_model_score_top_five_accurate = []\n",
    "    num_tasks_with_model_score_top_ten_accurate = []\n",
    "    ndcg_score_g_list = []\n",
    "    ndcg_model_score_list = []\n",
    "    has_gt_list = []\n",
    "    has_gt_in_candidate = []\n",
    "    # candidate_eval_data = candidate_eval_data[:1]\n",
    "    for i, row in candidate_eval_data.iterrows():\n",
    "        #print(i)\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[(eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) > 0\n",
    "        if np.nan not in set(c_e_data['GT_kg_id']):\n",
    "            has_gt_list.append(1)\n",
    "        else:\n",
    "            has_gt_list.append(0)\n",
    "        if 1 in set(c_e_data['evaluation_label']):\n",
    "            has_gt_in_candidate.append(1)\n",
    "        else:\n",
    "            has_gt_in_candidate.append(0)\n",
    "            \n",
    "        # handle graph-embedding-score\n",
    "        s_data = c_e_data.sort_values(by=['graph-embedding-score'], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_graph_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_graph_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_graph_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_graph_top_ten_accurate.append(0)\n",
    "        \n",
    "        #rank on model score\n",
    "        s_data = c_e_data.sort_values(by=[method], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(0)\n",
    "            \n",
    "        cf_e_data = c_e_data.copy()\n",
    "        #cf_e_data['evaluation_label'] = cf_e_data['evaluation_label'].replace(-1, 0)\n",
    "#         cf_e_data['text-embedding-score'] = cf_e_data['text-embedding-score'].replace(np.nan, 0)\n",
    "        cf_e_data['graph-embedding-score'] = cf_e_data['graph-embedding-score'].replace(np.nan, 0)\n",
    "        cf_e_data[method] = cf_e_data[method].replace(np.nan, 0)\n",
    "\n",
    "    candidate_eval_data['graph_top_one_accurate'] = num_tasks_with_graph_top_one_accurate\n",
    "    candidate_eval_data['graph_top_five_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['graph_top_ten_accurate'] = num_tasks_with_graph_top_five_accurate\n",
    "    candidate_eval_data['model_top_one_accurate'] = num_tasks_with_model_score_top_one_accurate\n",
    "    candidate_eval_data['model_top_five_accurate'] = num_tasks_with_model_score_top_five_accurate\n",
    "    candidate_eval_data['model_top_ten_accurate'] = num_tasks_with_model_score_top_ten_accurate\n",
    "    candidate_eval_data['has_gt'] = has_gt_list\n",
    "    candidate_eval_data['has_gt_in_candidate'] = has_gt_in_candidate\n",
    "    res['num_tasks_with_graph_top_one_accurate'] = sum(num_tasks_with_graph_top_one_accurate)\n",
    "    res['num_tasks_with_graph_top_five_accurate'] = sum(num_tasks_with_graph_top_five_accurate)\n",
    "    res['num_tasks_with_graph_top_ten_accurate'] = sum(num_tasks_with_graph_top_ten_accurate)\n",
    "    res['num_tasks_with_model_score_top_one_accurate'] = sum(num_tasks_with_model_score_top_one_accurate)\n",
    "    res['num_tasks_with_model_score_top_five_accurate'] = sum(num_tasks_with_model_score_top_five_accurate)\n",
    "    res['num_tasks_with_model_score_top_ten_accurate'] = sum(num_tasks_with_model_score_top_ten_accurate)\n",
    "    return res, candidate_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "quiet-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_tasks': 6684, 'num_tasks_with_gt': 5822, 'num_tasks_with_gt_in_candidate': 5620, 'num_tasks_with_singleton_candidate': 0, 'num_tasks_with_singleton_candidate_with_gt': 0, 'num_tasks_with_graph_top_one_accurate': 1954, 'num_tasks_with_graph_top_five_accurate': 3438, 'num_tasks_with_graph_top_ten_accurate': 3978, 'num_tasks_with_model_score_top_one_accurate': 4574, 'num_tasks_with_model_score_top_five_accurate': 5184, 'num_tasks_with_model_score_top_ten_accurate': 5312}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_id</th>\n",
       "      <th>row</th>\n",
       "      <th>column</th>\n",
       "      <th>count</th>\n",
       "      <th>graph_top_one_accurate</th>\n",
       "      <th>graph_top_five_accurate</th>\n",
       "      <th>graph_top_ten_accurate</th>\n",
       "      <th>model_top_one_accurate</th>\n",
       "      <th>model_top_five_accurate</th>\n",
       "      <th>model_top_ten_accurate</th>\n",
       "      <th>has_gt</th>\n",
       "      <th>has_gt_in_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10579449_0_1681126353774891032</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10579449_0_1681126353774891032</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10579449_0_1681126353774891032</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10579449_0_1681126353774891032</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10579449_0_1681126353774891032</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>99070098_0_2074872741302696997</td>\n",
       "      <td>209</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680</th>\n",
       "      <td>99070098_0_2074872741302696997</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>99070098_0_2074872741302696997</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6682</th>\n",
       "      <td>99070098_0_2074872741302696997</td>\n",
       "      <td>212</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6683</th>\n",
       "      <td>99070098_0_2074872741302696997</td>\n",
       "      <td>213</td>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6684 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            table_id  row  column  count  \\\n",
       "0     10579449_0_1681126353774891032    0       1    101   \n",
       "1     10579449_0_1681126353774891032    1       1    101   \n",
       "2     10579449_0_1681126353774891032    2       1    101   \n",
       "3     10579449_0_1681126353774891032    3       1    100   \n",
       "4     10579449_0_1681126353774891032    4       1     50   \n",
       "...                              ...  ...     ...    ...   \n",
       "6679  99070098_0_2074872741302696997  209       1    101   \n",
       "6680  99070098_0_2074872741302696997  210       1    101   \n",
       "6681  99070098_0_2074872741302696997  211       1    124   \n",
       "6682  99070098_0_2074872741302696997  212       1    107   \n",
       "6683  99070098_0_2074872741302696997  213       1    121   \n",
       "\n",
       "      graph_top_one_accurate  graph_top_five_accurate  graph_top_ten_accurate  \\\n",
       "0                          0                        1                       1   \n",
       "1                          0                        0                       0   \n",
       "2                          0                        0                       0   \n",
       "3                          0                        0                       0   \n",
       "4                          1                        1                       1   \n",
       "...                      ...                      ...                     ...   \n",
       "6679                       0                        1                       1   \n",
       "6680                       1                        1                       1   \n",
       "6681                       0                        1                       1   \n",
       "6682                       1                        1                       1   \n",
       "6683                       0                        0                       0   \n",
       "\n",
       "      model_top_one_accurate  model_top_five_accurate  model_top_ten_accurate  \\\n",
       "0                          1                        1                       1   \n",
       "1                          1                        1                       1   \n",
       "2                          0                        0                       0   \n",
       "3                          0                        0                       0   \n",
       "4                          1                        1                       1   \n",
       "...                      ...                      ...                     ...   \n",
       "6679                       1                        1                       1   \n",
       "6680                       1                        1                       1   \n",
       "6681                       0                        1                       1   \n",
       "6682                       1                        1                       1   \n",
       "6683                       0                        1                       1   \n",
       "\n",
       "      has_gt  has_gt_in_candidate  \n",
       "0          1                    1  \n",
       "1          1                    1  \n",
       "2          0                    0  \n",
       "3          0                    0  \n",
       "4          1                    1  \n",
       "...      ...                  ...  \n",
       "6679       1                    1  \n",
       "6680       1                    1  \n",
       "6681       1                    1  \n",
       "6682       1                    1  \n",
       "6683       1                    1  \n",
       "\n",
       "[6684 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res, candidate_eval_data = parse_eval_files_stats(all_data,'rf_model_pred')\n",
    "print(res)\n",
    "display(candidate_eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "younger-hearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tasks: 6684\n",
      "number of tasks with ground truth: 5822\n",
      "number of tasks with ground truth in candidate set: 5620, which is 96.53040192373756%\n",
      "number of tasks has singleton candidate set: 0, which is 0.0%\n",
      "number of tasks has singleton candidate set which is ground truth: 0, which is 0.0%\n",
      "\n",
      "number of tasks with top-1 accuracy in terms of graph embedding score: 1954, which is 33.56234970800413%\n",
      "number of tasks with top-5 accuracy in terms of graph embedding score: 3438, which is 59.05187220886293%\n",
      "number of tasks with top-10 accuracy in terms of graph embedding score: 3978, which is 68.3270353830299%\n",
      "\n",
      "number of tasks with top-1 accuracy in terms of model score: 4574, which is 78.56406733081415%\n",
      "number of tasks with top-5 accuracy in terms of model score: 5184, which is 89.04156647200276%\n",
      "number of tasks with top-10 accuracy in terms of model score: 5312, which is 91.24012366884232%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conclusion of exact-match on all tasks with ground truth (no filtering)\n",
    "print(f\"number of tasks: {res['num_tasks']}\")\n",
    "print(f\"number of tasks with ground truth: {res['num_tasks_with_gt']}\")\n",
    "print(f\"number of tasks with ground truth in candidate set: {res['num_tasks_with_gt_in_candidate']}, which is {res['num_tasks_with_gt_in_candidate']/res['num_tasks_with_gt'] * 100}%\")\n",
    "print(f\"number of tasks has singleton candidate set: {res['num_tasks_with_singleton_candidate']}, which is {res['num_tasks_with_singleton_candidate']/res['num_tasks_with_gt'] * 100}%\")\n",
    "print(f\"number of tasks has singleton candidate set which is ground truth: {res['num_tasks_with_singleton_candidate_with_gt']}, which is {res['num_tasks_with_singleton_candidate_with_gt']/res['num_tasks_with_gt'] * 100}%\")\n",
    "print()\n",
    "print(f\"number of tasks with top-1 accuracy in terms of graph embedding score: {res['num_tasks_with_graph_top_one_accurate']}, which is {res['num_tasks_with_graph_top_one_accurate']/res['num_tasks_with_gt'] * 100}%\")\n",
    "print(f\"number of tasks with top-5 accuracy in terms of graph embedding score: {res['num_tasks_with_graph_top_five_accurate']}, which is {res['num_tasks_with_graph_top_five_accurate']/res['num_tasks_with_gt'] * 100}%\")\n",
    "print(f\"number of tasks with top-10 accuracy in terms of graph embedding score: {res['num_tasks_with_graph_top_ten_accurate']}, which is {res['num_tasks_with_graph_top_ten_accurate']/res['num_tasks_with_gt'] * 100}%\")\n",
    "print()\n",
    "print(f\"number of tasks with top-1 accuracy in terms of model score: {res['num_tasks_with_model_score_top_one_accurate']}, which is {res['num_tasks_with_model_score_top_one_accurate']/res['num_tasks_with_gt'] * 100}%\")\n",
    "print(f\"number of tasks with top-5 accuracy in terms of model score: {res['num_tasks_with_model_score_top_five_accurate']}, which is {res['num_tasks_with_model_score_top_five_accurate']/res['num_tasks_with_gt'] * 100}%\")\n",
    "print(f\"number of tasks with top-10 accuracy in terms of model score: {res['num_tasks_with_model_score_top_ten_accurate']}, which is {res['num_tasks_with_model_score_top_ten_accurate']/res['num_tasks_with_gt'] * 100}%\")\n",
    "print()\n",
    "candidate_eval_data_with_gt = candidate_eval_data[candidate_eval_data['has_gt'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "vietnamese-married",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graph_top_one_accurate</th>\n",
       "      <th>model_top_one_accurate</th>\n",
       "      <th>graph_top_five_accurate</th>\n",
       "      <th>model_top_five_accurate</th>\n",
       "      <th>table type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>table_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14067031_0_559833072073397908</th>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>country II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14380604_4_3329235705746762392</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28086084_0_3127660530989916727</th>\n",
       "      <td>0.236607</td>\n",
       "      <td>0.611607</td>\n",
       "      <td>0.522321</td>\n",
       "      <td>0.723214</td>\n",
       "      <td>pope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29414811_2_4773219892816395776</th>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>video games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39759273_0_1427898308030295194</th>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45073662_0_3179937335063201739</th>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>players I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50270082_0_444360818941411589</th>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.744048</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>players II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84575189_0_6365692015941409487</th>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>magazines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                graph_top_one_accurate  \\\n",
       "table_id                                                 \n",
       "14067031_0_559833072073397908                 0.886792   \n",
       "14380604_4_3329235705746762392                0.700000   \n",
       "28086084_0_3127660530989916727                0.236607   \n",
       "29414811_2_4773219892816395776                0.136364   \n",
       "39759273_0_1427898308030295194                0.580000   \n",
       "45073662_0_3179937335063201739                0.407407   \n",
       "50270082_0_444360818941411589                 0.547619   \n",
       "84575189_0_6365692015941409487                0.110000   \n",
       "\n",
       "                                model_top_one_accurate  \\\n",
       "table_id                                                 \n",
       "14067031_0_559833072073397908                 0.943396   \n",
       "14380604_4_3329235705746762392                0.700000   \n",
       "28086084_0_3127660530989916727                0.611607   \n",
       "29414811_2_4773219892816395776                0.545455   \n",
       "39759273_0_1427898308030295194                0.880000   \n",
       "45073662_0_3179937335063201739                0.629630   \n",
       "50270082_0_444360818941411589                 0.744048   \n",
       "84575189_0_6365692015941409487                0.700000   \n",
       "\n",
       "                                graph_top_five_accurate  \\\n",
       "table_id                                                  \n",
       "14067031_0_559833072073397908                  0.943396   \n",
       "14380604_4_3329235705746762392                 0.900000   \n",
       "28086084_0_3127660530989916727                 0.522321   \n",
       "29414811_2_4773219892816395776                 0.500000   \n",
       "39759273_0_1427898308030295194                 0.930000   \n",
       "45073662_0_3179937335063201739                 0.888889   \n",
       "50270082_0_444360818941411589                  0.922619   \n",
       "84575189_0_6365692015941409487                 0.230000   \n",
       "\n",
       "                                model_top_five_accurate   table type  \n",
       "table_id                                                              \n",
       "14067031_0_559833072073397908                  0.943396   country II  \n",
       "14380604_4_3329235705746762392                 0.800000    companies  \n",
       "28086084_0_3127660530989916727                 0.723214         pope  \n",
       "29414811_2_4773219892816395776                 0.863636  video games  \n",
       "39759273_0_1427898308030295194                 0.970000       movies  \n",
       "45073662_0_3179937335063201739                 0.740741    players I  \n",
       "50270082_0_444360818941411589                  0.910714   players II  \n",
       "84575189_0_6365692015941409487                 0.780000    magazines  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = candidate_eval_data.groupby(['table_id']).agg({\n",
    "    'graph_top_one_accurate':lambda x: sum(x)/len(x),\n",
    "    'model_top_one_accurate':lambda x: sum(x)/len(x),\n",
    "    'graph_top_five_accurate':lambda x: sum(x)/len(x),\n",
    "    'model_top_five_accurate':lambda x: sum(x)/len(x)\n",
    "})\n",
    "c['table type'] = [\n",
    "    'country II',\n",
    "    'companies',\n",
    "    'pope',\n",
    "    'video games',\n",
    "    'movies',\n",
    "    'players I',\n",
    "    'players II',\n",
    "    'magazines'\n",
    "]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-antique",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-aggregate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-cattle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-worship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_env",
   "language": "python",
   "name": "tl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
